{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85042a7c",
   "metadata": {
    "executionInfo": {
     "elapsed": 7896,
     "status": "ok",
     "timestamp": 1740080508521,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "85042a7c"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eiq8z8_j4WQ0",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1740081075140,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "eiq8z8_j4WQ0"
   },
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "save_data_here = os.path.join(current_path, \"data_fmnist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa478c4",
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1740081076324,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "3fa478c4"
   },
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "train_data = datasets.FashionMNIST(root = save_data_here, train = True, download = True, transform = transforms)\n",
    "test_data = datasets.FashionMNIST(root = save_data_here, train = False, download = True, transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2a5be8",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740081179066,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "ba2a5be8"
   },
   "outputs": [],
   "source": [
    "split = int((len(train_data)*0.8))\n",
    "\n",
    "train_set, eval_set = torch.utils.data.random_split(train_data,[split,len(train_data)-split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283ef1e4",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740081180844,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "283ef1e4"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 64, shuffle = True)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_set, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53af8a4a-485d-4757-ac4b-5e022b617a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "print(len(train_loader))\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, label = batch\n",
    "    print(images.size())\n",
    "    print(label.size())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f93f1e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740081182097,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "1f93f1e0"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, kernel_size = 3, stride=1, padding=1)     #PADDING=1 (32x28x28)   //PADDING=0output_size = ((H - kernel) / stride) + 1 = ((28 - 3)/1) + 1 = 26); 26x32=21632\n",
    "        self.conv2 = nn.Conv2d(32,64, kernel_size = 3, stride=1, padding=1)    #(64x28x28)\n",
    "        #max pool is included in forward process (64x14x14)\n",
    "        self.conv3 = nn.Conv2d(64,128, kernel_size = 3, stride=1, padding=1)   #(128x14x14)\n",
    "        self.conv4 = nn.Conv2d(128,256, kernel_size = 3, stride=1, padding=1)  #(256x14x14)\n",
    "        #another max pool --> (256x7x7)\n",
    "        self.fc1 = nn.Linear(256 * 7 * 7, 512)                            \n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "        #maxpool layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Reduces size by half\n",
    "\n",
    "        #dropout layers\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        #x = torch.flatten(x,1)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(x, 1)     #This option is the good one to run over bathcs, it does not flatten batch size whoch is 0 position\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f84712",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740081184059,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "f1f84712"
   },
   "outputs": [],
   "source": [
    " fmodel = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c795202-9cab-4e0a-89de-d90ec3a7662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the kernels from conv1 layer (shape: [32, 1, 3, 3] for 32 filters of size 3x3)\n",
    "kernels = fmodel.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "# Plot the first 32 kernels\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "for i in range(32):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    ax.imshow(kernels[i, 0], cmap='gray')  # Plot the first (and only) channel of each kernel\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a33689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1740081184524,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "16a33689",
    "outputId": "1f78ff97-0882-425e-d419-3317559fc684",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x49 and 12544x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m fmodel(x[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#print(x.size())\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x49 and 12544x512)"
     ]
    }
   ],
   "source": [
    "x = train_set.__getitem__(0)\n",
    "\n",
    "fmodel(x[0])\n",
    "\n",
    "#This gives error since we are not considering batches, therefore flattening function, flattens 7,7 leaving 256 unflattened\n",
    "#When looping over batches (64,256,7,7) the function flattens to (64, 12544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e93fe",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740079517374,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "9c8e93fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63ce39",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740081192217,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "cc63ce39"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3cd6a9",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740081193841,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "fd3cd6a9"
   },
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(fmodel.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(fmodel.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85ca87f-d7bd-45de-89ae-d026f3be5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9db46c8",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740081376842,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "a9db46c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "\n",
    "    global epoch_train_loss\n",
    "    global train_losses\n",
    "    train_counter = []\n",
    "    train_loss = []\n",
    "    epoch_train_loss = 0.0\n",
    "    save_model_here = os.path.join(current_path, \"RESULTADOS/fmodel.pth\")\n",
    "    fmodel.train() #no hay capas que desactivar\n",
    "\n",
    "    for batch_idx,data_target in enumerate(train_loader):\n",
    "        data,target = data_target\n",
    "        optimizer.zero_grad()\n",
    "        output = fmodel(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        if( batch_idx % 10 == 0):\n",
    "            print(\"Training epoch {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(epoch, batch_idx*len(data), 48000, (batch_idx*len(data))/(len(train_set))*100, loss.item()))\n",
    "            train_loss.append(loss.item())\n",
    "            train_counter.append(batch_idx * len(data) + (epoch - 1) * len(train_set))\n",
    "            torch.save(fmodel.state_dict(), save_model_here)\n",
    "            torch.save(optimizer.state_dict(), save_model_here)\n",
    "            \n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3979d021",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740081382844,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "3979d021"
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "\n",
    "    prediction_list = []\n",
    "    correct = 0\n",
    "    eval_loss = 0\n",
    "\n",
    "    fmodel.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in eval_loader:\n",
    "            output = fmodel(data)\n",
    "#           prediction = output.max(1 , keepdim = True)[1]\n",
    "            prediction = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "            prediction_list.append(prediction)\n",
    "            correct += (prediction == target).sum().item()\n",
    "\n",
    "            #loss summatory for Average loss\n",
    "            eval_loss += criterion(output, target)\n",
    "\n",
    "        accuracy = correct/12000\n",
    "        eval_loss /= len(eval_loader)\n",
    "        print(eval_loss)\n",
    "        eval_losses.append(eval_loss)\n",
    "\n",
    "        print(\"\\nEval: Average loss: {:.4f},\\tAccuracy: {}/{} ({:.0f}%)\".format(eval_loss, correct, len(eval_loader.dataset), accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be803460",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 5185,
     "status": "error",
     "timestamp": 1740079841372,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "be803460",
    "outputId": "923e3925-bde4-4fb6-bd40-63ef159eb6a7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1: [0/48000 (0%)]\tLoss: 0.329971\n",
      "Training epoch 1: [640/48000 (1%)]\tLoss: 0.405135\n",
      "Training epoch 1: [1280/48000 (3%)]\tLoss: 0.493786\n",
      "Training epoch 1: [1920/48000 (4%)]\tLoss: 0.294408\n",
      "Training epoch 1: [2560/48000 (5%)]\tLoss: 0.534906\n",
      "Training epoch 1: [3200/48000 (7%)]\tLoss: 0.301316\n",
      "Training epoch 1: [3840/48000 (8%)]\tLoss: 0.318311\n",
      "Training epoch 1: [4480/48000 (9%)]\tLoss: 0.243204\n",
      "Training epoch 1: [5120/48000 (11%)]\tLoss: 0.419888\n",
      "Training epoch 1: [5760/48000 (12%)]\tLoss: 0.158317\n",
      "Training epoch 1: [6400/48000 (13%)]\tLoss: 0.265122\n",
      "Training epoch 1: [7040/48000 (15%)]\tLoss: 0.280979\n",
      "Training epoch 1: [7680/48000 (16%)]\tLoss: 0.337290\n",
      "Training epoch 1: [8320/48000 (17%)]\tLoss: 0.177400\n",
      "Training epoch 1: [8960/48000 (19%)]\tLoss: 0.378501\n",
      "Training epoch 1: [9600/48000 (20%)]\tLoss: 0.386765\n",
      "Training epoch 1: [10240/48000 (21%)]\tLoss: 0.303876\n",
      "Training epoch 1: [10880/48000 (23%)]\tLoss: 0.272015\n",
      "Training epoch 1: [11520/48000 (24%)]\tLoss: 0.251527\n",
      "Training epoch 1: [12160/48000 (25%)]\tLoss: 0.311674\n",
      "Training epoch 1: [12800/48000 (27%)]\tLoss: 0.358488\n",
      "Training epoch 1: [13440/48000 (28%)]\tLoss: 0.291623\n",
      "Training epoch 1: [14080/48000 (29%)]\tLoss: 0.385681\n",
      "Training epoch 1: [14720/48000 (31%)]\tLoss: 0.339132\n",
      "Training epoch 1: [15360/48000 (32%)]\tLoss: 0.290224\n",
      "Training epoch 1: [16000/48000 (33%)]\tLoss: 0.220284\n",
      "Training epoch 1: [16640/48000 (35%)]\tLoss: 0.279885\n",
      "Training epoch 1: [17280/48000 (36%)]\tLoss: 0.301746\n",
      "Training epoch 1: [17920/48000 (37%)]\tLoss: 0.313174\n",
      "Training epoch 1: [18560/48000 (39%)]\tLoss: 0.326682\n",
      "Training epoch 1: [19200/48000 (40%)]\tLoss: 0.397176\n",
      "Training epoch 1: [19840/48000 (41%)]\tLoss: 0.409948\n",
      "Training epoch 1: [20480/48000 (43%)]\tLoss: 0.474379\n",
      "Training epoch 1: [21120/48000 (44%)]\tLoss: 0.141572\n",
      "Training epoch 1: [21760/48000 (45%)]\tLoss: 0.230557\n",
      "Training epoch 1: [22400/48000 (47%)]\tLoss: 0.271526\n",
      "Training epoch 1: [23040/48000 (48%)]\tLoss: 0.551663\n",
      "Training epoch 1: [23680/48000 (49%)]\tLoss: 0.298946\n",
      "Training epoch 1: [24320/48000 (51%)]\tLoss: 0.291074\n",
      "Training epoch 1: [24960/48000 (52%)]\tLoss: 0.374064\n",
      "Training epoch 1: [25600/48000 (53%)]\tLoss: 0.264714\n",
      "Training epoch 1: [26240/48000 (55%)]\tLoss: 0.403747\n",
      "Training epoch 1: [26880/48000 (56%)]\tLoss: 0.275680\n",
      "Training epoch 1: [27520/48000 (57%)]\tLoss: 0.376935\n",
      "Training epoch 1: [28160/48000 (59%)]\tLoss: 0.266503\n",
      "Training epoch 1: [28800/48000 (60%)]\tLoss: 0.264841\n",
      "Training epoch 1: [29440/48000 (61%)]\tLoss: 0.223900\n",
      "Training epoch 1: [30080/48000 (63%)]\tLoss: 0.219654\n",
      "Training epoch 1: [30720/48000 (64%)]\tLoss: 0.412342\n",
      "Training epoch 1: [31360/48000 (65%)]\tLoss: 0.394564\n",
      "Training epoch 1: [32000/48000 (67%)]\tLoss: 0.430132\n",
      "Training epoch 1: [32640/48000 (68%)]\tLoss: 0.373103\n",
      "Training epoch 1: [33280/48000 (69%)]\tLoss: 0.311986\n",
      "Training epoch 1: [33920/48000 (71%)]\tLoss: 0.346527\n",
      "Training epoch 1: [34560/48000 (72%)]\tLoss: 0.366261\n",
      "Training epoch 1: [35200/48000 (73%)]\tLoss: 0.292097\n",
      "Training epoch 1: [35840/48000 (75%)]\tLoss: 0.515711\n",
      "Training epoch 1: [36480/48000 (76%)]\tLoss: 0.309056\n",
      "Training epoch 1: [37120/48000 (77%)]\tLoss: 0.227998\n",
      "Training epoch 1: [37760/48000 (79%)]\tLoss: 0.209907\n",
      "Training epoch 1: [38400/48000 (80%)]\tLoss: 0.299385\n",
      "Training epoch 1: [39040/48000 (81%)]\tLoss: 0.365909\n",
      "Training epoch 1: [39680/48000 (83%)]\tLoss: 0.354090\n",
      "Training epoch 1: [40320/48000 (84%)]\tLoss: 0.407805\n",
      "Training epoch 1: [40960/48000 (85%)]\tLoss: 0.404617\n",
      "Training epoch 1: [41600/48000 (87%)]\tLoss: 0.272601\n",
      "Training epoch 1: [42240/48000 (88%)]\tLoss: 0.260320\n",
      "Training epoch 1: [42880/48000 (89%)]\tLoss: 0.339297\n",
      "Training epoch 1: [43520/48000 (91%)]\tLoss: 0.471177\n",
      "Training epoch 1: [44160/48000 (92%)]\tLoss: 0.325259\n",
      "Training epoch 1: [44800/48000 (93%)]\tLoss: 0.383054\n",
      "Training epoch 1: [45440/48000 (95%)]\tLoss: 0.337729\n",
      "Training epoch 1: [46080/48000 (96%)]\tLoss: 0.275267\n",
      "Training epoch 1: [46720/48000 (97%)]\tLoss: 0.320461\n",
      "Training epoch 1: [47360/48000 (99%)]\tLoss: 0.376780\n",
      "tensor(0.3151)\n",
      "\n",
      "Eval: Average loss: 0.3151,\tAccuracy: 10650/12000 (89%)\n",
      "Training epoch 2: [0/48000 (0%)]\tLoss: 0.417258\n",
      "Training epoch 2: [640/48000 (1%)]\tLoss: 0.376834\n",
      "Training epoch 2: [1280/48000 (3%)]\tLoss: 0.323190\n",
      "Training epoch 2: [1920/48000 (4%)]\tLoss: 0.407852\n",
      "Training epoch 2: [2560/48000 (5%)]\tLoss: 0.229254\n",
      "Training epoch 2: [3200/48000 (7%)]\tLoss: 0.456362\n",
      "Training epoch 2: [3840/48000 (8%)]\tLoss: 0.313054\n",
      "Training epoch 2: [4480/48000 (9%)]\tLoss: 0.409078\n",
      "Training epoch 2: [5120/48000 (11%)]\tLoss: 0.269421\n",
      "Training epoch 2: [5760/48000 (12%)]\tLoss: 0.260104\n",
      "Training epoch 2: [6400/48000 (13%)]\tLoss: 0.272214\n",
      "Training epoch 2: [7040/48000 (15%)]\tLoss: 0.456840\n",
      "Training epoch 2: [7680/48000 (16%)]\tLoss: 0.432790\n",
      "Training epoch 2: [8320/48000 (17%)]\tLoss: 0.384017\n",
      "Training epoch 2: [8960/48000 (19%)]\tLoss: 0.512753\n",
      "Training epoch 2: [9600/48000 (20%)]\tLoss: 0.317446\n",
      "Training epoch 2: [10240/48000 (21%)]\tLoss: 0.322616\n",
      "Training epoch 2: [10880/48000 (23%)]\tLoss: 0.243054\n",
      "Training epoch 2: [11520/48000 (24%)]\tLoss: 0.316246\n",
      "Training epoch 2: [12160/48000 (25%)]\tLoss: 0.274226\n",
      "Training epoch 2: [12800/48000 (27%)]\tLoss: 0.336494\n",
      "Training epoch 2: [13440/48000 (28%)]\tLoss: 0.228224\n",
      "Training epoch 2: [14080/48000 (29%)]\tLoss: 0.380728\n",
      "Training epoch 2: [14720/48000 (31%)]\tLoss: 0.421036\n",
      "Training epoch 2: [15360/48000 (32%)]\tLoss: 0.228450\n",
      "Training epoch 2: [16000/48000 (33%)]\tLoss: 0.471521\n",
      "Training epoch 2: [16640/48000 (35%)]\tLoss: 0.329169\n",
      "Training epoch 2: [17280/48000 (36%)]\tLoss: 0.494028\n",
      "Training epoch 2: [17920/48000 (37%)]\tLoss: 0.254890\n",
      "Training epoch 2: [18560/48000 (39%)]\tLoss: 0.242865\n",
      "Training epoch 2: [19200/48000 (40%)]\tLoss: 0.323768\n",
      "Training epoch 2: [19840/48000 (41%)]\tLoss: 0.276573\n",
      "Training epoch 2: [20480/48000 (43%)]\tLoss: 0.203576\n",
      "Training epoch 2: [21120/48000 (44%)]\tLoss: 0.406675\n",
      "Training epoch 2: [21760/48000 (45%)]\tLoss: 0.113593\n",
      "Training epoch 2: [22400/48000 (47%)]\tLoss: 0.358892\n",
      "Training epoch 2: [23040/48000 (48%)]\tLoss: 0.381342\n",
      "Training epoch 2: [23680/48000 (49%)]\tLoss: 0.350489\n",
      "Training epoch 2: [24320/48000 (51%)]\tLoss: 0.368696\n",
      "Training epoch 2: [24960/48000 (52%)]\tLoss: 0.410600\n",
      "Training epoch 2: [25600/48000 (53%)]\tLoss: 0.258971\n",
      "Training epoch 2: [26240/48000 (55%)]\tLoss: 0.184817\n",
      "Training epoch 2: [26880/48000 (56%)]\tLoss: 0.164569\n",
      "Training epoch 2: [27520/48000 (57%)]\tLoss: 0.399383\n",
      "Training epoch 2: [28160/48000 (59%)]\tLoss: 0.355774\n",
      "Training epoch 2: [28800/48000 (60%)]\tLoss: 0.444881\n",
      "Training epoch 2: [29440/48000 (61%)]\tLoss: 0.390920\n",
      "Training epoch 2: [30080/48000 (63%)]\tLoss: 0.319352\n",
      "Training epoch 2: [30720/48000 (64%)]\tLoss: 0.232106\n",
      "Training epoch 2: [31360/48000 (65%)]\tLoss: 0.301046\n",
      "Training epoch 2: [32000/48000 (67%)]\tLoss: 0.207926\n",
      "Training epoch 2: [32640/48000 (68%)]\tLoss: 0.374927\n",
      "Training epoch 2: [33280/48000 (69%)]\tLoss: 0.288958\n",
      "Training epoch 2: [33920/48000 (71%)]\tLoss: 0.330063\n",
      "Training epoch 2: [34560/48000 (72%)]\tLoss: 0.440255\n",
      "Training epoch 2: [35200/48000 (73%)]\tLoss: 0.394803\n",
      "Training epoch 2: [35840/48000 (75%)]\tLoss: 0.370912\n",
      "Training epoch 2: [36480/48000 (76%)]\tLoss: 0.395339\n",
      "Training epoch 2: [37120/48000 (77%)]\tLoss: 0.336711\n",
      "Training epoch 2: [37760/48000 (79%)]\tLoss: 0.132067\n",
      "Training epoch 2: [38400/48000 (80%)]\tLoss: 0.500595\n",
      "Training epoch 2: [39040/48000 (81%)]\tLoss: 0.384922\n",
      "Training epoch 2: [39680/48000 (83%)]\tLoss: 0.339022\n",
      "Training epoch 2: [40320/48000 (84%)]\tLoss: 0.360643\n",
      "Training epoch 2: [40960/48000 (85%)]\tLoss: 0.260087\n",
      "Training epoch 2: [41600/48000 (87%)]\tLoss: 0.330205\n",
      "Training epoch 2: [42240/48000 (88%)]\tLoss: 0.303809\n",
      "Training epoch 2: [42880/48000 (89%)]\tLoss: 0.524390\n",
      "Training epoch 2: [43520/48000 (91%)]\tLoss: 0.308371\n",
      "Training epoch 2: [44160/48000 (92%)]\tLoss: 0.481989\n",
      "Training epoch 2: [44800/48000 (93%)]\tLoss: 0.271023\n",
      "Training epoch 2: [45440/48000 (95%)]\tLoss: 0.351172\n",
      "Training epoch 2: [46080/48000 (96%)]\tLoss: 0.277599\n",
      "Training epoch 2: [46720/48000 (97%)]\tLoss: 0.277257\n",
      "Training epoch 2: [47360/48000 (99%)]\tLoss: 0.455651\n",
      "tensor(0.3014)\n",
      "\n",
      "Eval: Average loss: 0.3014,\tAccuracy: 10720/12000 (89%)\n",
      "Training epoch 3: [0/48000 (0%)]\tLoss: 0.302598\n",
      "Training epoch 3: [640/48000 (1%)]\tLoss: 0.403005\n",
      "Training epoch 3: [1280/48000 (3%)]\tLoss: 0.353116\n",
      "Training epoch 3: [1920/48000 (4%)]\tLoss: 0.254056\n",
      "Training epoch 3: [2560/48000 (5%)]\tLoss: 0.245339\n",
      "Training epoch 3: [3200/48000 (7%)]\tLoss: 0.391666\n",
      "Training epoch 3: [3840/48000 (8%)]\tLoss: 0.216305\n",
      "Training epoch 3: [4480/48000 (9%)]\tLoss: 0.255036\n",
      "Training epoch 3: [5120/48000 (11%)]\tLoss: 0.466556\n",
      "Training epoch 3: [5760/48000 (12%)]\tLoss: 0.404251\n",
      "Training epoch 3: [6400/48000 (13%)]\tLoss: 0.293348\n",
      "Training epoch 3: [7040/48000 (15%)]\tLoss: 0.263603\n",
      "Training epoch 3: [7680/48000 (16%)]\tLoss: 0.338767\n",
      "Training epoch 3: [8320/48000 (17%)]\tLoss: 0.201166\n",
      "Training epoch 3: [8960/48000 (19%)]\tLoss: 0.306239\n",
      "Training epoch 3: [9600/48000 (20%)]\tLoss: 0.247091\n",
      "Training epoch 3: [10240/48000 (21%)]\tLoss: 0.290296\n",
      "Training epoch 3: [10880/48000 (23%)]\tLoss: 0.566478\n",
      "Training epoch 3: [11520/48000 (24%)]\tLoss: 0.164801\n",
      "Training epoch 3: [12160/48000 (25%)]\tLoss: 0.449787\n",
      "Training epoch 3: [12800/48000 (27%)]\tLoss: 0.255851\n",
      "Training epoch 3: [13440/48000 (28%)]\tLoss: 0.686641\n",
      "Training epoch 3: [14080/48000 (29%)]\tLoss: 0.351046\n",
      "Training epoch 3: [14720/48000 (31%)]\tLoss: 0.263128\n",
      "Training epoch 3: [15360/48000 (32%)]\tLoss: 0.358927\n",
      "Training epoch 3: [16000/48000 (33%)]\tLoss: 0.303075\n",
      "Training epoch 3: [16640/48000 (35%)]\tLoss: 0.356894\n",
      "Training epoch 3: [17280/48000 (36%)]\tLoss: 0.347993\n",
      "Training epoch 3: [17920/48000 (37%)]\tLoss: 0.299491\n",
      "Training epoch 3: [18560/48000 (39%)]\tLoss: 0.163028\n",
      "Training epoch 3: [19200/48000 (40%)]\tLoss: 0.280592\n",
      "Training epoch 3: [19840/48000 (41%)]\tLoss: 0.466076\n",
      "Training epoch 3: [20480/48000 (43%)]\tLoss: 0.273177\n",
      "Training epoch 3: [21120/48000 (44%)]\tLoss: 0.340949\n",
      "Training epoch 3: [21760/48000 (45%)]\tLoss: 0.249401\n",
      "Training epoch 3: [22400/48000 (47%)]\tLoss: 0.443843\n",
      "Training epoch 3: [23040/48000 (48%)]\tLoss: 0.262124\n",
      "Training epoch 3: [23680/48000 (49%)]\tLoss: 0.197522\n",
      "Training epoch 3: [24320/48000 (51%)]\tLoss: 0.221599\n",
      "Training epoch 3: [24960/48000 (52%)]\tLoss: 0.323329\n",
      "Training epoch 3: [25600/48000 (53%)]\tLoss: 0.381224\n",
      "Training epoch 3: [26240/48000 (55%)]\tLoss: 0.338046\n",
      "Training epoch 3: [26880/48000 (56%)]\tLoss: 0.331806\n",
      "Training epoch 3: [27520/48000 (57%)]\tLoss: 0.437924\n",
      "Training epoch 3: [28160/48000 (59%)]\tLoss: 0.234630\n",
      "Training epoch 3: [28800/48000 (60%)]\tLoss: 0.318715\n",
      "Training epoch 3: [29440/48000 (61%)]\tLoss: 0.304357\n",
      "Training epoch 3: [30080/48000 (63%)]\tLoss: 0.354763\n",
      "Training epoch 3: [30720/48000 (64%)]\tLoss: 0.426975\n",
      "Training epoch 3: [31360/48000 (65%)]\tLoss: 0.279597\n",
      "Training epoch 3: [32000/48000 (67%)]\tLoss: 0.271186\n",
      "Training epoch 3: [32640/48000 (68%)]\tLoss: 0.185068\n",
      "Training epoch 3: [33280/48000 (69%)]\tLoss: 0.281242\n",
      "Training epoch 3: [33920/48000 (71%)]\tLoss: 0.252468\n",
      "Training epoch 3: [34560/48000 (72%)]\tLoss: 0.260897\n",
      "Training epoch 3: [35200/48000 (73%)]\tLoss: 0.160264\n",
      "Training epoch 3: [35840/48000 (75%)]\tLoss: 0.352152\n",
      "Training epoch 3: [36480/48000 (76%)]\tLoss: 0.172028\n",
      "Training epoch 3: [37120/48000 (77%)]\tLoss: 0.329996\n",
      "Training epoch 3: [37760/48000 (79%)]\tLoss: 0.353713\n",
      "Training epoch 3: [38400/48000 (80%)]\tLoss: 0.417290\n",
      "Training epoch 3: [39040/48000 (81%)]\tLoss: 0.297156\n",
      "Training epoch 3: [39680/48000 (83%)]\tLoss: 0.467796\n",
      "Training epoch 3: [40320/48000 (84%)]\tLoss: 0.299287\n",
      "Training epoch 3: [40960/48000 (85%)]\tLoss: 0.294786\n",
      "Training epoch 3: [41600/48000 (87%)]\tLoss: 0.337037\n",
      "Training epoch 3: [42240/48000 (88%)]\tLoss: 0.224385\n",
      "Training epoch 3: [42880/48000 (89%)]\tLoss: 0.173146\n",
      "Training epoch 3: [43520/48000 (91%)]\tLoss: 0.320495\n",
      "Training epoch 3: [44160/48000 (92%)]\tLoss: 0.167168\n",
      "Training epoch 3: [44800/48000 (93%)]\tLoss: 0.292455\n",
      "Training epoch 3: [45440/48000 (95%)]\tLoss: 0.275874\n",
      "Training epoch 3: [46080/48000 (96%)]\tLoss: 0.389693\n",
      "Training epoch 3: [46720/48000 (97%)]\tLoss: 0.326380\n",
      "Training epoch 3: [47360/48000 (99%)]\tLoss: 0.375638\n",
      "tensor(0.2987)\n",
      "\n",
      "Eval: Average loss: 0.2987,\tAccuracy: 10714/12000 (89%)\n",
      "Training epoch 4: [0/48000 (0%)]\tLoss: 0.274471\n",
      "Training epoch 4: [640/48000 (1%)]\tLoss: 0.393371\n",
      "Training epoch 4: [1280/48000 (3%)]\tLoss: 0.247293\n",
      "Training epoch 4: [1920/48000 (4%)]\tLoss: 0.315885\n",
      "Training epoch 4: [2560/48000 (5%)]\tLoss: 0.226842\n",
      "Training epoch 4: [3200/48000 (7%)]\tLoss: 0.278174\n",
      "Training epoch 4: [3840/48000 (8%)]\tLoss: 0.369730\n",
      "Training epoch 4: [4480/48000 (9%)]\tLoss: 0.245319\n",
      "Training epoch 4: [5120/48000 (11%)]\tLoss: 0.225374\n",
      "Training epoch 4: [5760/48000 (12%)]\tLoss: 0.221364\n",
      "Training epoch 4: [6400/48000 (13%)]\tLoss: 0.303755\n",
      "Training epoch 4: [7040/48000 (15%)]\tLoss: 0.559565\n",
      "Training epoch 4: [7680/48000 (16%)]\tLoss: 0.423198\n",
      "Training epoch 4: [8320/48000 (17%)]\tLoss: 0.268791\n",
      "Training epoch 4: [8960/48000 (19%)]\tLoss: 0.240912\n",
      "Training epoch 4: [9600/48000 (20%)]\tLoss: 0.174689\n",
      "Training epoch 4: [10240/48000 (21%)]\tLoss: 0.283325\n",
      "Training epoch 4: [10880/48000 (23%)]\tLoss: 0.268653\n",
      "Training epoch 4: [11520/48000 (24%)]\tLoss: 0.373197\n",
      "Training epoch 4: [12160/48000 (25%)]\tLoss: 0.647600\n",
      "Training epoch 4: [12800/48000 (27%)]\tLoss: 0.328040\n",
      "Training epoch 4: [13440/48000 (28%)]\tLoss: 0.244953\n",
      "Training epoch 4: [14080/48000 (29%)]\tLoss: 0.197761\n",
      "Training epoch 4: [14720/48000 (31%)]\tLoss: 0.216172\n",
      "Training epoch 4: [15360/48000 (32%)]\tLoss: 0.395382\n",
      "Training epoch 4: [16000/48000 (33%)]\tLoss: 0.256774\n",
      "Training epoch 4: [16640/48000 (35%)]\tLoss: 0.269973\n",
      "Training epoch 4: [17280/48000 (36%)]\tLoss: 0.210924\n",
      "Training epoch 4: [17920/48000 (37%)]\tLoss: 0.342727\n",
      "Training epoch 4: [18560/48000 (39%)]\tLoss: 0.337839\n",
      "Training epoch 4: [19200/48000 (40%)]\tLoss: 0.248344\n",
      "Training epoch 4: [19840/48000 (41%)]\tLoss: 0.310542\n",
      "Training epoch 4: [20480/48000 (43%)]\tLoss: 0.281845\n",
      "Training epoch 4: [21120/48000 (44%)]\tLoss: 0.208285\n",
      "Training epoch 4: [21760/48000 (45%)]\tLoss: 0.296361\n",
      "Training epoch 4: [22400/48000 (47%)]\tLoss: 0.458965\n",
      "Training epoch 4: [23040/48000 (48%)]\tLoss: 0.412247\n",
      "Training epoch 4: [23680/48000 (49%)]\tLoss: 0.382473\n",
      "Training epoch 4: [24320/48000 (51%)]\tLoss: 0.289594\n",
      "Training epoch 4: [24960/48000 (52%)]\tLoss: 0.346547\n",
      "Training epoch 4: [25600/48000 (53%)]\tLoss: 0.330327\n",
      "Training epoch 4: [26240/48000 (55%)]\tLoss: 0.427486\n",
      "Training epoch 4: [26880/48000 (56%)]\tLoss: 0.323690\n",
      "Training epoch 4: [27520/48000 (57%)]\tLoss: 0.329272\n",
      "Training epoch 4: [28160/48000 (59%)]\tLoss: 0.193449\n",
      "Training epoch 4: [28800/48000 (60%)]\tLoss: 0.252453\n",
      "Training epoch 4: [29440/48000 (61%)]\tLoss: 0.279381\n",
      "Training epoch 4: [30080/48000 (63%)]\tLoss: 0.345648\n",
      "Training epoch 4: [30720/48000 (64%)]\tLoss: 0.274137\n",
      "Training epoch 4: [31360/48000 (65%)]\tLoss: 0.451319\n",
      "Training epoch 4: [32000/48000 (67%)]\tLoss: 0.135032\n",
      "Training epoch 4: [32640/48000 (68%)]\tLoss: 0.209067\n",
      "Training epoch 4: [33280/48000 (69%)]\tLoss: 0.231928\n",
      "Training epoch 4: [33920/48000 (71%)]\tLoss: 0.228653\n",
      "Training epoch 4: [34560/48000 (72%)]\tLoss: 0.537477\n",
      "Training epoch 4: [35200/48000 (73%)]\tLoss: 0.203514\n",
      "Training epoch 4: [35840/48000 (75%)]\tLoss: 0.162437\n",
      "Training epoch 4: [36480/48000 (76%)]\tLoss: 0.313341\n",
      "Training epoch 4: [37120/48000 (77%)]\tLoss: 0.259534\n",
      "Training epoch 4: [37760/48000 (79%)]\tLoss: 0.284091\n",
      "Training epoch 4: [38400/48000 (80%)]\tLoss: 0.419391\n",
      "Training epoch 4: [39040/48000 (81%)]\tLoss: 0.331907\n",
      "Training epoch 4: [39680/48000 (83%)]\tLoss: 0.274842\n",
      "Training epoch 4: [40320/48000 (84%)]\tLoss: 0.182201\n",
      "Training epoch 4: [40960/48000 (85%)]\tLoss: 0.321943\n",
      "Training epoch 4: [41600/48000 (87%)]\tLoss: 0.310342\n",
      "Training epoch 4: [42240/48000 (88%)]\tLoss: 0.306153\n",
      "Training epoch 4: [42880/48000 (89%)]\tLoss: 0.281273\n",
      "Training epoch 4: [43520/48000 (91%)]\tLoss: 0.266581\n",
      "Training epoch 4: [44160/48000 (92%)]\tLoss: 0.248676\n",
      "Training epoch 4: [44800/48000 (93%)]\tLoss: 0.237684\n",
      "Training epoch 4: [45440/48000 (95%)]\tLoss: 0.215242\n",
      "Training epoch 4: [46080/48000 (96%)]\tLoss: 0.336734\n",
      "Training epoch 4: [46720/48000 (97%)]\tLoss: 0.244203\n",
      "Training epoch 4: [47360/48000 (99%)]\tLoss: 0.098789\n",
      "tensor(0.2965)\n",
      "\n",
      "Eval: Average loss: 0.2965,\tAccuracy: 10716/12000 (89%)\n",
      "Training epoch 5: [0/48000 (0%)]\tLoss: 0.407510\n",
      "Training epoch 5: [640/48000 (1%)]\tLoss: 0.213642\n",
      "Training epoch 5: [1280/48000 (3%)]\tLoss: 0.423894\n",
      "Training epoch 5: [1920/48000 (4%)]\tLoss: 0.227335\n",
      "Training epoch 5: [2560/48000 (5%)]\tLoss: 0.210257\n",
      "Training epoch 5: [3200/48000 (7%)]\tLoss: 0.264695\n",
      "Training epoch 5: [3840/48000 (8%)]\tLoss: 0.311907\n",
      "Training epoch 5: [4480/48000 (9%)]\tLoss: 0.337117\n",
      "Training epoch 5: [5120/48000 (11%)]\tLoss: 0.343680\n",
      "Training epoch 5: [5760/48000 (12%)]\tLoss: 0.282009\n",
      "Training epoch 5: [6400/48000 (13%)]\tLoss: 0.354548\n",
      "Training epoch 5: [7040/48000 (15%)]\tLoss: 0.315739\n",
      "Training epoch 5: [7680/48000 (16%)]\tLoss: 0.335554\n",
      "Training epoch 5: [8320/48000 (17%)]\tLoss: 0.250438\n",
      "Training epoch 5: [8960/48000 (19%)]\tLoss: 0.444518\n",
      "Training epoch 5: [9600/48000 (20%)]\tLoss: 0.462377\n",
      "Training epoch 5: [10240/48000 (21%)]\tLoss: 0.327254\n",
      "Training epoch 5: [10880/48000 (23%)]\tLoss: 0.274444\n",
      "Training epoch 5: [11520/48000 (24%)]\tLoss: 0.254940\n",
      "Training epoch 5: [12160/48000 (25%)]\tLoss: 0.426595\n",
      "Training epoch 5: [12800/48000 (27%)]\tLoss: 0.350141\n",
      "Training epoch 5: [13440/48000 (28%)]\tLoss: 0.262890\n",
      "Training epoch 5: [14080/48000 (29%)]\tLoss: 0.448009\n",
      "Training epoch 5: [14720/48000 (31%)]\tLoss: 0.176133\n",
      "Training epoch 5: [15360/48000 (32%)]\tLoss: 0.259883\n",
      "Training epoch 5: [16000/48000 (33%)]\tLoss: 0.271080\n",
      "Training epoch 5: [16640/48000 (35%)]\tLoss: 0.261138\n",
      "Training epoch 5: [17280/48000 (36%)]\tLoss: 0.462855\n",
      "Training epoch 5: [17920/48000 (37%)]\tLoss: 0.289706\n",
      "Training epoch 5: [18560/48000 (39%)]\tLoss: 0.257239\n",
      "Training epoch 5: [19200/48000 (40%)]\tLoss: 0.248383\n",
      "Training epoch 5: [19840/48000 (41%)]\tLoss: 0.242083\n",
      "Training epoch 5: [20480/48000 (43%)]\tLoss: 0.221571\n",
      "Training epoch 5: [21120/48000 (44%)]\tLoss: 0.398380\n",
      "Training epoch 5: [21760/48000 (45%)]\tLoss: 0.370667\n",
      "Training epoch 5: [22400/48000 (47%)]\tLoss: 0.288586\n",
      "Training epoch 5: [23040/48000 (48%)]\tLoss: 0.419091\n",
      "Training epoch 5: [23680/48000 (49%)]\tLoss: 0.179525\n",
      "Training epoch 5: [24320/48000 (51%)]\tLoss: 0.415635\n",
      "Training epoch 5: [24960/48000 (52%)]\tLoss: 0.356331\n",
      "Training epoch 5: [25600/48000 (53%)]\tLoss: 0.252016\n",
      "Training epoch 5: [26240/48000 (55%)]\tLoss: 0.430597\n",
      "Training epoch 5: [26880/48000 (56%)]\tLoss: 0.399093\n",
      "Training epoch 5: [27520/48000 (57%)]\tLoss: 0.166010\n",
      "Training epoch 5: [28160/48000 (59%)]\tLoss: 0.178723\n",
      "Training epoch 5: [28800/48000 (60%)]\tLoss: 0.296396\n",
      "Training epoch 5: [29440/48000 (61%)]\tLoss: 0.356052\n",
      "Training epoch 5: [30080/48000 (63%)]\tLoss: 0.342435\n",
      "Training epoch 5: [30720/48000 (64%)]\tLoss: 0.257200\n",
      "Training epoch 5: [31360/48000 (65%)]\tLoss: 0.335950\n",
      "Training epoch 5: [32000/48000 (67%)]\tLoss: 0.261770\n",
      "Training epoch 5: [32640/48000 (68%)]\tLoss: 0.234239\n",
      "Training epoch 5: [33280/48000 (69%)]\tLoss: 0.314472\n",
      "Training epoch 5: [33920/48000 (71%)]\tLoss: 0.247407\n",
      "Training epoch 5: [34560/48000 (72%)]\tLoss: 0.284890\n",
      "Training epoch 5: [35200/48000 (73%)]\tLoss: 0.176944\n",
      "Training epoch 5: [35840/48000 (75%)]\tLoss: 0.252437\n",
      "Training epoch 5: [36480/48000 (76%)]\tLoss: 0.285209\n",
      "Training epoch 5: [37120/48000 (77%)]\tLoss: 0.298008\n",
      "Training epoch 5: [37760/48000 (79%)]\tLoss: 0.412726\n",
      "Training epoch 5: [38400/48000 (80%)]\tLoss: 0.602143\n",
      "Training epoch 5: [39040/48000 (81%)]\tLoss: 0.263314\n",
      "Training epoch 5: [39680/48000 (83%)]\tLoss: 0.226933\n",
      "Training epoch 5: [40320/48000 (84%)]\tLoss: 0.244816\n",
      "Training epoch 5: [40960/48000 (85%)]\tLoss: 0.322655\n",
      "Training epoch 5: [41600/48000 (87%)]\tLoss: 0.149360\n",
      "Training epoch 5: [42240/48000 (88%)]\tLoss: 0.265623\n",
      "Training epoch 5: [42880/48000 (89%)]\tLoss: 0.274297\n",
      "Training epoch 5: [43520/48000 (91%)]\tLoss: 0.429765\n",
      "Training epoch 5: [44160/48000 (92%)]\tLoss: 0.237262\n",
      "Training epoch 5: [44800/48000 (93%)]\tLoss: 0.279905\n",
      "Training epoch 5: [45440/48000 (95%)]\tLoss: 0.339956\n",
      "Training epoch 5: [46080/48000 (96%)]\tLoss: 0.320171\n",
      "Training epoch 5: [46720/48000 (97%)]\tLoss: 0.293961\n",
      "Training epoch 5: [47360/48000 (99%)]\tLoss: 0.313765\n",
      "tensor(0.2886)\n",
      "\n",
      "Eval: Average loss: 0.2886,\tAccuracy: 10725/12000 (89%)\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b79b8c8c",
   "metadata": {
    "executionInfo": {
     "elapsed": 13242,
     "status": "aborted",
     "timestamp": 1740079523284,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "b79b8c8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train and Evaluation loss during epochs')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABj7ElEQVR4nO3deXwTZeIG8CfpkTQ90ote0JaCSDkrZykIgkDlXBFdWA8OBVlWYEWW3y6IB+iuFVdd8QBEgS4eFV3OXfEAOQqCXFLwKAgKtEBLaUubnkmTzO+PaaZNL3okmaR9vp/PfJKZvJm8E6N9fK9RCIIggIiIiKgNUcpdASIiIiJHYwAiIiKiNocBiIiIiNocBiAiIiJqcxiAiIiIqM1hACIiIqI2hwGIiIiI2hwGICIiImpzGICIiIiozWEAIrtTKBSN2vbv39+iz1m+fDkUCoVtKu1gycnJUCgUuHTpUoPlLNdY33ar97dEx44dMXPmTLudHwAOHz6M5cuXo6CgoNZrw4cPx/Dhw+36+XW5dOkSFAoFkpOTHf7ZzTFz5kx07NjRZc7bGsycORM+Pj5yV4OayF3uClDrd+TIEav9F198Efv27cPevXutjnfv3r1FnzN79myMGTOmRedwFV9++SW0Wm2t4+Hh4TLUxnYOHz6MFStWYObMmfD397d6bfXq1fJUigAAzz77LJ588km5q0FkMwxAZHeDBg2y2m/Xrh2USmWt4zWVlpZCo9E0+nM6dOiADh06NKuOrqZfv34IDg6WuxoO1dKATM1j+fewc+fOcleFyKbYBUZOYfjw4ejZsydSU1MxePBgaDQaPPbYYwCAzZs3IzExEeHh4fDy8kK3bt2wZMkSlJSUWJ2jri6wjh07YsKECfjyyy/Rt29feHl5ITY2Fhs2bGhUvVasWIH4+HgEBgbCz88Pffv2xfr161HzHsJN+ZzvvvsOQ4YMgVqtRkREBJYuXYqKioqmfF0NqqioQEhICKZNm1brtYKCAnh5eWHRokUAgPLycvzlL3/BHXfcAa1Wi8DAQCQkJGDHjh23/Jz6uu32799fq0tz9+7duPfee9GhQweo1Wrcdttt+OMf/4jc3FypzPLly/F///d/AICYmJhaXaN1dYHl5+fjiSeeQPv27eHp6YlOnTph2bJl0Ov1VuUUCgXmz5+PDz74AN26dYNGo0FcXBz+97//3fI663Po0CGMHDkSvr6+0Gg0GDx4MD7//HOrMqWlpVi8eDFiYmKgVqsRGBiI/v37IyUlRSrz22+/4Q9/+AMiIiKgUqkQGhqKkSNHIi0t7ZZ1SE5ORteuXaFSqdCtWzds2rSpVpm6/nkAdXftWbpyfvjhByQmJsLX1xcjR46UXqvZBdaU73XHjh3o3bs3VCoVOnXqhFWrVjWp23rPnj0YOXIk/Pz8oNFoMGTIEHzzzTdWZSznO3XqFCZPngw/Pz9otVo88sgjuHHjhlVZs9mMV155BbGxsVCpVAgJCcH06dNx5cqVWp/95ZdfYuTIkdBqtdBoNOjWrRuSkpJqlbtw4QLGjRsHHx8fREZG4i9/+Uut3+KaNWsQFxcHHx8f+Pr6IjY2Fk8//XSjvgOyLbYAkdPIysrCI488gr/+9a946aWXoFSK+fz8+fMYN24cFi5cCG9vb5w9exYrV67EsWPHanWj1eX06dP4y1/+giVLliA0NBTvv/8+Zs2ahdtuuw3Dhg1r8L2XLl3CH//4R0RFRQEQw8uCBQtw9epVPPfcc03+nJ9//hkjR45Ex44dkZycDI1Gg9WrV+Pjjz9u0ndlMplgNBqtjikUCri5ucHDwwOPPPII1q5di3feeQd+fn5SmZSUFJSXl+PRRx8FAOj1euTn52Px4sVo3749DAYD9uzZg8mTJ2Pjxo2YPn16k+pVn19//RUJCQmYPXs2tFotLl26hNdffx133nknfvjhB3h4eGD27NnIz8/HW2+9ha1bt0rdefW1/JSXl2PEiBH49ddfsWLFCvTu3RsHDx5EUlIS0tLSaoWRzz//HMePH8cLL7wAHx8fvPLKK7jvvvtw7tw5dOrUqUnXc+DAAYwePRq9e/fG+vXroVKpsHr1akycOBEpKSmYOnUqAGDRokX44IMP8Pe//x19+vRBSUkJfvzxR+Tl5UnnGjduHEwmE1555RVERUUhNzcXhw8frnMcVHXJycl49NFHce+99+K1115DYWEhli9fDr1eL/270xwGgwG/+93v8Mc//hFLliyp9TurqTHf65dffonJkydj2LBh2Lx5M4xGI1599VVcv369UXX68MMPMX36dNx7773497//DQ8PD7z77ru455578NVXX0khzeK+++7DlClTMHfuXPz000949tln8fPPP+Po0aPw8PAAAPzpT3/CunXrMH/+fEyYMAGXLl3Cs88+i/379+P777+XWljXr1+Pxx9/HHfddRfWrl2LkJAQ/PLLL/jxxx+tPrOiogK/+93vMGvWLPzlL39BamoqXnzxRWi1Wum/FZ988gmeeOIJLFiwAK+++iqUSiUuXLiAn3/+uVHfA9mYQORgM2bMELy9va2O3XXXXQIA4ZtvvmnwvWazWaioqBAOHDggABBOnz4tvfb8888LNX/S0dHRglqtFi5fviwdKysrEwIDA4U//vGPTaq3yWQSKioqhBdeeEEICgoSzGZzkz9n6tSpgpeXl5CdnS0dMxqNQmxsrABAuHjxYoN1sFxjXVvnzp2lcmfOnBEACOvWrbN6/8CBA4V+/frVe36j0ShUVFQIs2bNEvr06WP1WnR0tDBjxgxpf+PGjXXWed++fQIAYd++fXV+huWf4eXLlwUAwo4dO6TX/vnPf9b7Pdx1113CXXfdJe2vXbtWACB8+umnVuVWrlwpABC+/vpr6RgAITQ0VNDpdNKx7OxsQalUCklJSfV8G6KLFy8KAISNGzdKxwYNGiSEhIQIRUVF0jGj0Sj07NlT6NChg/Tb6NmzpzBp0qR6z52bmysAEN54440G61CTyWQSIiIihL59+1r9Di9duiR4eHgI0dHR0rH6/nnUdV0zZswQAAgbNmyo9ZkzZsywOq8gNP57HTBggBAZGSno9XrpWFFRkRAUFFTr39maSkpKhMDAQGHixIm1voO4uDhh4MCB0jHLvx9PPfWUVdmPPvpIACB8+OGHgiAIQnp6ugBAeOKJJ6zKHT16VAAgPP3001Id/fz8hDvvvNPqe67J8r3V/C2OGzdO6Nq1q7Q/f/58wd/fv8HrJcdhFxg5jYCAANx99921jv/222946KGHEBYWJrVw3HXXXQCA9PT0W573jjvukFpwAECtVuP222/H5cuXb/nevXv3YtSoUdBqtdJnP/fcc8jLy0NOTk6TP2ffvn0YOXIkQkNDpWNubm5Si0Fj7dmzB8ePH7fatm/fLr3eq1cv9OvXDxs3bpSOpaen49ixY1LXosVnn32GIUOGwMfHB+7u7vDw8MD69esb9d02Vk5ODubOnYvIyEjpM6Kjo6V6NcfevXvh7e2NBx54wOq4ZaZaze6RESNGwNfXV9oPDQ1FSEhIo34H1ZWUlODo0aN44IEHrGb+uLm5Ydq0abhy5QrOnTsHABg4cCC++OILLFmyBPv370dZWZnVuQIDA9G5c2f885//xOuvv45Tp07BbDbfsg7nzp3DtWvX8NBDD1l1IUVHR2Pw4MFNup663H///Y0ue6vvtaSkBCdOnMCkSZPg6ekplfPx8cHEiRNvef7Dhw8jPz8fM2bMgNFolDaz2YwxY8bg+PHjtbrDH374Yav9KVOmwN3dHfv27QMA6bHmrMaBAweiW7du0m/n8OHD0Ol0eOKJJ27ZVadQKGpdT+/eva1+XwMHDkRBQQEefPBB7Nixw6oLmByPAYicRl0zmIqLizF06FAcPXoUf//737F//34cP34cW7duBYBaf1DqEhQUVOuYSqW65XuPHTuGxMREAMB7772Hb7/9FsePH8eyZcvq/OzGfE5eXh7CwsJqlavrWEPi4uLQv39/q61nz55WZR577DEcOXIEZ8+eBQBs3LgRKpUKDz74oFRm69atmDJlCtq3b48PP/wQR44cwfHjx/HYY4+hvLy8SXWqj9lsRmJiIrZu3Yq//vWv+Oabb3Ds2DF89913ABr3z7Aulu+y5h+mkJAQuLu7W3UzAc3/HdR08+ZNCIJQ5+81IiJCqhsAvPnmm/jb3/6G7du3Y8SIEQgMDMSkSZNw/vx5AOIfzW+++Qb33HMPXnnlFfTt2xft2rXDn//8ZxQVFTV47UDdv5um/pZq0mg0Vt2mt3Kr79XyfVUP/RZ1HavJ0k32wAMPwMPDw2pbuXIlBEFAfn6+1Xtqfgfu7u4ICgqSvjfLY33/DC2vW8YNNWZyhUajgVqttjqmUqms/j2aNm0aNmzYgMuXL+P+++9HSEgI4uPjsXv37luen2yPY4DIadT1f1h79+7FtWvXsH//fqnVB8Atx0fYwieffAIPDw/873//s/oPW/WWlqYKCgpCdnZ2reN1HWupBx98EIsWLUJycjL+8Y9/4IMPPsCkSZMQEBAglfnwww8RExODzZs3W33/NQdu1sXyndQsW/P/an/88UecPn0aycnJmDFjhnT8woULzboui6CgIBw9ehSCIFjVPScnB0aj0W6z5AICAqBUKpGVlVXrtWvXrgGA9Nne3t5YsWIFVqxYgevXr0utQRMnTpSCaXR0NNavXw8A+OWXX/Dpp59i+fLlMBgMWLt2bZ11sISOxvyWGvvPycLWa2kFBARAoVDUOd6nMb97y3f51ltv1TtztGaQys7ORvv27aV9o9GIvLw86XuzPGZlZdUKN9euXZM+s127dgBQ58Do5nr00Ufx6KOPoqSkBKmpqXj++ecxYcIE/PLLL1KrKDkGW4DIqVn+Y6xSqayOv/vuuw75bHd3d7i5uUnHysrK8MEHHzT7nCNGjMA333xj9cfAZDJh8+bNLaprXQICAjBp0iRs2rQJ//vf/5CdnV2r+0uhUMDT09Pqj152dnajZoFZZgSdOXPG6vjOnTtrfQbQuH+GljKNaZUZOXIkiouLawVSy0yomgNjbcXb2xvx8fHYunWrVT3NZjM+/PBDdOjQAbfffnut94WGhmLmzJl48MEHce7cOZSWltYqc/vtt+OZZ55Br1698P3339dbh65duyI8PBwpKSlWMxIvX76Mw4cPW5Vt7D8ne/H29kb//v2xfft2GAwG6XhxcXGjZuENGTIE/v7++Pnnn2u1elq26l1rAPDRRx9Z7X/66acwGo3SLEJLV/uHH35oVe748eNIT0+XfjuDBw+GVqvF2rVra838bClvb2+MHTsWy5Ytg8FgwE8//WTT89OtsQWInNrgwYMREBCAuXPn4vnnn4eHhwc++ugjnD592u6fPX78eLz++ut46KGHMGfOHOTl5eHVV1+t9Ye8KZ555hns3LkTd999N5577jloNBq88847tcYw3MrJkyfrXAixe/fuVt0Xjz32GDZv3oz58+ejQ4cOGDVqlFX5CRMmYOvWrXjiiSfwwAMPIDMzEy+++CLCw8Olbpr6DBgwAF27dsXixYthNBoREBCAbdu24dChQ1blYmNj0blzZyxZsgSCICAwMBD//e9/62z279WrFwBg1apVmDFjBjw8PNC1a1erMSYW06dPxzvvvIMZM2bg0qVL6NWrFw4dOoSXXnoJ48aNq3WttpSUlITRo0djxIgRWLx4MTw9PbF69Wr8+OOPSElJkUJffHw8JkyYgN69eyMgIADp6en44IMPkJCQAI1GgzNnzmD+/Pn4/e9/jy5dusDT0xN79+7FmTNnsGTJkno/X6lU4sUXX8Ts2bNx33334fHHH0dBQQGWL19eq/snLCwMo0aNQlJSEgICAhAdHY1vvvlG6kZ2hBdeeAHjx4/HPffcgyeffBImkwn//Oc/4ePjU6v7qiYfHx+89dZbmDFjBvLz8/HAAw8gJCQEN27cwOnTp3Hjxg2sWbPG6j1bt26Fu7s7Ro8eLc0Ci4uLw5QpUwCIAXLOnDl46623oFQqMXbsWGkWWGRkJJ566inps1977TXMnj0bo0aNwuOPP47Q0FBcuHABp0+fxttvv92k7+Hxxx+Hl5cXhgwZgvDwcGRnZyMpKQlarRYDBgxo0rnIBmQcgE1tVH2zwHr06FFn+cOHDwsJCQmCRqMR2rVrJ8yePVv4/vvva81gqW8W2Pjx42uds+aMovps2LBB6Nq1q6BSqYROnToJSUlJwvr162vNVGrK53z77bfCoEGDBJVKJYSFhQn/93//J6xbt67Fs8AACLt377YqbzKZhMjISAGAsGzZsjrP+fLLLwsdO3YUVCqV0K1bN+G9996r97usPgtMEAThl19+ERITEwU/Pz+hXbt2woIFC4TPP/+81qyjn3/+WRg9erTg6+srBAQECL///e+FjIwMAYDw/PPPW51z6dKlQkREhKBUKq3OU9d3mZeXJ8ydO1cIDw8X3N3dhejoaGHp0qVCeXm5VTkAwrx582pde13XVFNds6UEQRAOHjwo3H333YK3t7fg5eUlDBo0SPjvf/9rVWbJkiVC//79hYCAAOk39NRTTwm5ubmCIAjC9evXhZkzZwqxsbGCt7e34OPjI/Tu3Vv417/+JRiNxgbrJQiC8P777wtdunQRPD09hdtvv13YsGFDnbO1srKyhAceeEAIDAwUtFqt8MgjjwgnTpyocxZYzX83q79W1yywxn6v27ZtE3r16iV4enoKUVFRwssvvyz8+c9/FgICAm55nYIgCAcOHBDGjx8vBAYGCh4eHkL79u2F8ePHC5999plUxvK7PXnypDBx4kTBx8dH8PX1FR588EHh+vXrVuczmUzCypUrhdtvv13w8PAQgoODhUceeUTIzMys9dm7du0S7rrrLsHb21vQaDRC9+7dhZUrV1p9N3V9bzX/Pfr3v/8tjBgxQggNDRU8PT2FiIgIYcqUKcKZM2ca9R2QbSkEwcbtekRERLdQUVGBO+64A+3bt8fXX39tk3MuX74cK1aswI0bN9rcSunUdOwCIyIiu5s1axZGjx4tdf2sXbsW6enpWLVqldxVozaKAYiIiOyuqKgIixcvxo0bN+Dh4YG+ffti165ddh2rRdQQdoERERFRm8Np8ERERNTmMAARERFRm8MARERERG0OB0HXwWw249q1a/D19bX5svBERERkH4IgoKioCBEREVAqG27jYQCqw7Vr1xAZGSl3NYiIiKgZMjMzb3kTWwagOliW3c/MzGzSXZGJiIhIPjqdDpGRkXXePqcmBqA6WLq9/Pz8GICIiIhcTGOGr3AQNBEREbU5DEBERETU5jAAERERUZvDAERERERtDgMQERERtTkMQERERNTmyBqAkpKSMGDAAPj6+iIkJASTJk3CuXPnGnzP1q1bMXr0aLRr1w5+fn5ISEjAV199ZVUmOTkZCoWi1lZeXm7PyyEiIiIXIWsAOnDgAObNm4fvvvsOu3fvhtFoRGJiIkpKSup9T2pqKkaPHo1du3bh5MmTGDFiBCZOnIhTp05ZlfPz80NWVpbVplar7X1JRERE5AIUgiAIclfC4saNGwgJCcGBAwcwbNiwRr+vR48emDp1Kp577jkAYgvQwoULUVBQ0Kx66HQ6aLVaFBYWciFEIiIiF9GUv99ONQaosLAQABAYGNjo95jNZhQVFdV6T3FxMaKjo9GhQwdMmDChVgtRdXq9HjqdzmojIiKi1stpApAgCFi0aBHuvPNO9OzZs9Hve+2111BSUoIpU6ZIx2JjY5GcnIydO3ciJSUFarUaQ4YMwfnz5+s8R1JSErRarbTxRqhEREStm9N0gc2bNw+ff/45Dh06dMs7uFqkpKRg9uzZ2LFjB0aNGlVvObPZjL59+2LYsGF48803a72u1+uh1+ulfcvN1NgFRkRE5Dqa0gXmFDdDXbBgAXbu3InU1NRGh5/Nmzdj1qxZ+OyzzxoMPwCgVCoxYMCAeluAVCoVVCpVk+vdVEaTGfklBpRVmBAd5G33zyMiIqK6ydoFJggC5s+fj61bt2Lv3r2IiYlp1PtSUlIwc+ZMfPzxxxg/fnyjPictLQ3h4eEtrXKLHL2Yj4EvfYPHN52QtR5ERERtnawtQPPmzcPHH3+MHTt2wNfXF9nZ2QAArVYLLy8vAMDSpUtx9epVbNq0CYAYfqZPn45Vq1Zh0KBB0nu8vLyg1WoBACtWrMCgQYPQpUsX6HQ6vPnmm0hLS8M777wjw1VWCfLxBADkFRtkrQcREVFbJ2sL0Jo1a1BYWIjhw4cjPDxc2jZv3iyVycrKQkZGhrT/7rvvwmg0Yt68eVbvefLJJ6UyBQUFmDNnDrp164bExERcvXoVqampGDhwoEOvr6Ygb7GbLb/UAKPJLGtdiIiI2jKnGQTtTOy1DpDJLOC2ZbsgCMDxZaPQztf+446IiIjaCpddB6i1c1MqEKip7AYr0d+iNBEREdkLA5CDcRwQERGR/BiAHMwyDii3mC1AREREcmEAcjBLC1AuW4CIiIhkwwDkYME+YgtQHluAiIiIZMMA5GDBHANEREQkOwYgBwuytABxFhgREZFsGIAcLMibY4CIiIjkxgDkYJYWIM4CIyIikg8DkIO1kwZBswWIiIhILgxADmaZBl9WYUKpwShzbYiIiNomBiAH03i6Qe0hfu1sBSIiIpIHA5CDKRQKrgZNREQkMwYgGQRzNWgiIiJZMQDJgKtBExERyYsBSAbSHeFL2AJEREQkBwYgGXAtICIiInkxAMnAsho0Z4ERERHJgwFIBsFsASIiIpIVA5AMgrkaNBERkawYgGRQNQiaLUBERERyYACSgSUA5ZcYYDILMteGiIio7WEAkkGgRgxAZgEoKGU3GBERkaMxAMnA3U2JAI0HAK4GTUREJAcGIJlwNWgiIiL5MADJxDIOKJerQRMRETkcA5BMgtgCREREJBsGIJkEczVoIiIi2TAAyYT3AyMiIpIPA5BMqm6HwRYgIiIiR2MAkglXgyYiIpIPA5BMgn04BoiIiEgusgagpKQkDBgwAL6+vggJCcGkSZNw7ty5W77vwIED6NevH9RqNTp16oS1a9fWKrNlyxZ0794dKpUK3bt3x7Zt2+xxCc0W5M1ZYERERHKRNQAdOHAA8+bNw3fffYfdu3fDaDQiMTERJSUl9b7n4sWLGDduHIYOHYpTp07h6aefxp///Gds2bJFKnPkyBFMnToV06ZNw+nTpzFt2jRMmTIFR48edcRlNYqlC6zEYEKZwSRzbYiIiNoWhSAITnM3zhs3biAkJAQHDhzAsGHD6izzt7/9DTt37kR6erp0bO7cuTh9+jSOHDkCAJg6dSp0Oh2++OILqcyYMWMQEBCAlJSUW9ZDp9NBq9WisLAQfn5+LbyqugmCgK7PfgmD0YyDfx2ByECNXT6HiIiorWjK32+nGgNUWFgIAAgMDKy3zJEjR5CYmGh17J577sGJEydQUVHRYJnDhw/XeU69Xg+dTme12ZtCoUA7y2KIXA2aiIjIoZwmAAmCgEWLFuHOO+9Ez5496y2XnZ2N0NBQq2OhoaEwGo3Izc1tsEx2dnad50xKSoJWq5W2yMjIFl5N40gzwTgOiIiIyKGcJgDNnz8fZ86caVQXlUKhsNq39OJVP15XmZrHLJYuXYrCwkJpy8zMbGr1myWIq0ETERHJwl3uCgDAggULsHPnTqSmpqJDhw4Nlg0LC6vVkpOTkwN3d3cEBQU1WKZmq5CFSqWCSqVqwRU0j7QaNNcCIiIicihZW4AEQcD8+fOxdetW7N27FzExMbd8T0JCAnbv3m117Ouvv0b//v3h4eHRYJnBgwfbrvI2IN0RvogtQERERI4kawCaN28ePvzwQ3z88cfw9fVFdnY2srOzUVZWJpVZunQppk+fLu3PnTsXly9fxqJFi5Ceno4NGzZg/fr1WLx4sVTmySefxNdff42VK1fi7NmzWLlyJfbs2YOFCxc68vJuqWoQNFuAiIiIHEnWALRmzRoUFhZi+PDhCA8Pl7bNmzdLZbKyspCRkSHtx8TEYNeuXdi/fz/uuOMOvPjii3jzzTdx//33S2UGDx6MTz75BBs3bkTv3r2RnJyMzZs3Iz4+3qHXdytBXA2aiIhIFk61DpCzcMQ6QACQ+ssNTN9wDLFhvvhyYd3rHhEREVHjuOw6QG1N1Q1R2QJERETkSAxAMgquHAOUX2KA2cyGOCIiIkdhAJJRYOU6QCazgIKyCplrQ0RE1HYwAMnIw00Jf404dZ+rQRMRETkOA5DMLKtB53ImGBERkcMwAMksiGsBERERORwDkMyCpdWgGYCIiIgchQFIZsFSCxC7wIiIiByFAUhmQd6VN0TlGCAiIiKHYQCSWdXtMNgFRkRE5CgMQDIL5mrQREREDscAJDPLLLBctgARERE5DAOQzKRB0BwDRERE5DAMQDKzjAEq1htRXmGSuTZERERtAwOQzHxV7vB0E/8xcBwQERGRYzAAyUyhUHAmGBERkYMxADkBSwDiQGgiIiLHYAByAsE+XAyRiIjIkRiAnIBlNWjOBCMiInIMBiAnEMwxQERERA7FAOQEgrgaNBERkUMxADmBqhuisgWIiIjIERiAnEDVLDC2ABERETkCA5ATqLodBluAiIiIHIEByAlYAlB+iQFmsyBzbYiIiFo/BiAnEOgtdoEZzQJ05RUy14aIiKj1YwByAp7uSvip3QFwIDQREZEjMAA5Ca4GTURE5DgMQE6iaiA0AxAREZG9MQA5iarFENkFRkREZG8MQE6CawERERE5DgOQk+Bq0ERERI4jawBKTU3FxIkTERERAYVCge3btzdYfubMmVAoFLW2Hj16SGWSk5PrLFNeXm7nq2kZ3hCViIjIcWQNQCUlJYiLi8Pbb7/dqPKrVq1CVlaWtGVmZiIwMBC///3vrcr5+flZlcvKyoJarbbHJdgMB0ETERE5jrucHz527FiMHTu20eW1Wi20Wq20v337dty8eROPPvqoVTmFQoGwsDCb1dMRgiwBiHeEJyIisjuXHgO0fv16jBo1CtHR0VbHi4uLER0djQ4dOmDChAk4deqUTDVsvKpB0OwCIyIisjeXDUBZWVn44osvMHv2bKvjsbGxSE5Oxs6dO5GSkgK1Wo0hQ4bg/Pnz9Z5Lr9dDp9NZbY4WXDkIuqjcCL3R5PDPJyIiaktcNgAlJyfD398fkyZNsjo+aNAgPPLII4iLi8PQoUPx6aef4vbbb8dbb71V77mSkpKk7jWtVovIyEg71742Py93eLgpAHAcEBERkb25ZAASBAEbNmzAtGnT4Onp2WBZpVKJAQMGNNgCtHTpUhQWFkpbZmamrat8SwqFQpoKzwBERERkX7IOgm6uAwcO4MKFC5g1a9YtywqCgLS0NPTq1aveMiqVCiqVypZVbJYgH09k68qRy9WgiYiI7ErWAFRcXIwLFy5I+xcvXkRaWhoCAwMRFRWFpUuX4urVq9i0aZPV+9avX4/4+Hj07Nmz1jlXrFiBQYMGoUuXLtDpdHjzzTeRlpaGd955x+7X01JBnApPRETkELIGoBMnTmDEiBHS/qJFiwAAM2bMQHJyMrKyspCRkWH1nsLCQmzZsgWrVq2q85wFBQWYM2cOsrOzodVq0adPH6SmpmLgwIH2uxAbCfbmYohERESOoBAEQZC7Es5Gp9NBq9WisLAQfn5+Dvvcf3z+M947eBGPD43BsvHdHfa5RERErUFT/n675CDo1oqrQRMRETkGA5ATsYwByuVq0ERERHbFAOREgnhDVCIiIodgAHIiwVwHiIiIyCEYgJyI1AJUogfHphMREdkPA5ATsQSgCpMAXZlR5toQERG1XgxATkTl7gZftbg0E1eDJiIish8GICfDqfBERET2xwDkZIK4GjQREZHdMQA5Gcs4oFwGICIiIrthAHIyli6wXHaBERER2Q0DkJOR7gjPQdBERER2wwDkZIKl1aDZAkRERGQvDEBOJoirQRMREdkdA5CT4SBoIiIi+2MAcjLBDEBERER2xwDkZCyzwHTlRhiMZplrQ0RE1DoxADkZP7UH3JUKAEB+CccBERER2QMDkJNRKhUI9GY3GBERkT0xADmhIGkxRAYgIiIie2AAckJcC4iIiMi+GICcUDBXgyYiIrIrBiAnVHVHeLYAERER2QMDkBMK4g1RiYiI7IoByAlxNWgiIiL7YgByQtIgaI4BIiIisgsGICckDYJmFxgREZFdMAA5oaBqAUgQBJlrQ0RE1PowADkhyywwg8mMIr1R5toQERG1PgxATkjt4QYflTsAILeI44CIiIhsjQHISQVJA6E5DoiIiMjWGICcVNVAaLYAERER2RoDkJMKku4IzxYgIiIiW5M1AKWmpmLixImIiIiAQqHA9u3bGyy/f/9+KBSKWtvZs2etym3ZsgXdu3eHSqVC9+7dsW3bNjtehX0EcSo8ERGR3cgagEpKShAXF4e33367Se87d+4csrKypK1Lly7Sa0eOHMHUqVMxbdo0nD59GtOmTcOUKVNw9OhRW1ffrrgYIhERkf24y/nhY8eOxdixY5v8vpCQEPj7+9f52htvvIHRo0dj6dKlAIClS5fiwIEDeOONN5CSktKS6jpUVRcYAxAREZGtueQYoD59+iA8PBwjR47Evn37rF47cuQIEhMTrY7dc889OHz4cL3n0+v10Ol0Vpvcgn15Q1QiIiJ7cakAFB4ejnXr1mHLli3YunUrunbtipEjRyI1NVUqk52djdDQUKv3hYaGIjs7u97zJiUlQavVSltkZKTdrqGxgrw5C4yIiMheZO0Ca6quXbuia9eu0n5CQgIyMzPx6quvYtiwYdJxhUJh9T5BEGodq27p0qVYtGiRtK/T6WQPQcFcB4iIiMhuXKoFqC6DBg3C+fPnpf2wsLBarT05OTm1WoWqU6lU8PPzs9rkZpkFVlBagQqTWebaEBERtS4uH4BOnTqF8PBwaT8hIQG7d++2KvP1119j8ODBjq5ai/h7ecBNKbZa5bMViIiIyKZk7QIrLi7GhQsXpP2LFy8iLS0NgYGBiIqKwtKlS3H16lVs2rQJgDjDq2PHjujRowcMBgM+/PBDbNmyBVu2bJHO8eSTT2LYsGFYuXIl7r33XuzYsQN79uzBoUOHHH59LaFUKhDo7YkbRXrkFusR6qeWu0pERESthqwB6MSJExgxYoS0bxmHM2PGDCQnJyMrKwsZGRnS6waDAYsXL8bVq1fh5eWFHj164PPPP8e4ceOkMoMHD8Ynn3yCZ555Bs8++yw6d+6MzZs3Iz4+3nEXZiNBlQGIiyESERHZlkIQBEHuSjgbnU4HrVaLwsJCWccDPfL+URy6kIt/TY3DfX06yFYPIiIiV9CUv98uPwaoNZPuCM8WICIiIptiAHJilrWAbnAtICIiIptiAHJibAEiIiKyDwYgJ9bOh6tBExER2QMDkBML4mrQREREdsEA5MSCpBYgBiAiIiJbYgByYkHeYgvQjWI9uFoBERGR7TAAObHgyhYgg9GMYr1R5toQERG1HgxATszL0w3enm4A2A1GRERkSwxATk4aB1TCmWBERES2wgDk5CwzwXLZAkRERGQzDEBOzrIadC7XAiIiIrIZBiAnF8zVoImIiGyOAcjJBXM1aCIiIptjAHJy0hggrgZNRERkMwxATi6ILUBEREQ2xwDk5IK9OQuMiIjI1hiAnBxbgIiIiGyPAcjJWWaB3SytgNFklrk2RERErQMDkJPz13hCqRCf55eyG4yIiMgWGICcnJtSgUBvrgVERERkSwxALoCrQRMREdkWA5ALCOJq0ERERDbVrACUmZmJK1euSPvHjh3DwoULsW7dOptVjKpYVoNmCxAREZFtNCsAPfTQQ9i3bx8AIDs7G6NHj8axY8fw9NNP44UXXrBpBalaCxBXgyYiIrKJZgWgH3/8EQMHDgQAfPrpp+jZsycOHz6Mjz/+GMnJybasH4H3AyMiIrK1ZgWgiooKqFTiH+U9e/bgd7/7HQAgNjYWWVlZtqsdAQCCuBo0ERGRTTUrAPXo0QNr167FwYMHsXv3bowZMwYAcO3aNQQFBdm0gsTVoImIiGytWQFo5cqVePfddzF8+HA8+OCDiIuLAwDs3LlT6hoj27GsBs0WICIiIttwb86bhg8fjtzcXOh0OgQEBEjH58yZA41GY7PKkUgaA1SihyAIUCgUMteIiIjItTWrBaisrAx6vV4KP5cvX8Ybb7yBc+fOISQkxKYVpKpZYOUVZpQaTDLXhoiIyPU1KwDde++92LRpEwCgoKAA8fHxeO211zBp0iSsWbPGphUkQOPpDi8PNwBcDJGIiMgWmhWAvv/+ewwdOhQA8J///AehoaG4fPkyNm3ahDfffLPR50lNTcXEiRMREREBhUKB7du3N1h+69atGD16NNq1awc/Pz8kJCTgq6++siqTnJwMhUJRaysvL2/ydToTSyvQDQ6EJiIiarFmBaDS0lL4+voCAL7++mtMnjwZSqUSgwYNwuXLlxt9npKSEsTFxeHtt99uVPnU1FSMHj0au3btwsmTJzFixAhMnDgRp06dsirn5+eHrKwsq02tVjf+Ap0Q1wIiIiKynWYNgr7tttuwfft23Hffffjqq6/w1FNPAQBycnLg5+fX6POMHTsWY8eObXT5N954w2r/pZdewo4dO/Df//4Xffr0kY4rFAqEhYU1+ryuIJirQRMREdlMs1qAnnvuOSxevBgdO3bEwIEDkZCQAEBsDaoeROzNbDajqKgIgYGBVseLi4sRHR2NDh06YMKECbVaiGrS6/XQ6XRWm7Ox3BGeLUBEREQt16wA9MADDyAjIwMnTpywGoMzcuRI/Otf/7JZ5W7ltddeQ0lJCaZMmSIdi42NRXJyMnbu3ImUlBSo1WoMGTIE58+fr/c8SUlJ0Gq10hYZGemI6jdJENcCIiIishmFIAhCS05w5coVKBQKtG/fvmUVUSiwbds2TJo0qVHlU1JSMHv2bOzYsQOjRo2qt5zZbEbfvn0xbNiwegdo6/V66PVVLSs6nQ6RkZEoLCxsUpeePa0/dBEv/u9nTOgdjrcf6it3dYiIiJyOTqeDVqtt1N/vZrUAmc1mvPDCC9BqtYiOjkZUVBT8/f3x4osvwmw2N6vSTbF582bMmjULn376aYPhBwCUSiUGDBjQYAuQSqWCn5+f1eZspDFAbAEiIiJqsWYNgl62bBnWr1+Pl19+GUOGDIEgCPj222+xfPlylJeX4x//+Iet6ylJSUnBY489hpSUFIwfP/6W5QVBQFpaGnr16mW3OjVaTjpw9F3A0xu4p2nfUfXVoImIiKhlmhWA/v3vf+P999+X7gIPAHFxcWjfvj2eeOKJRgeg4uJiXLhwQdq/ePEi0tLSEBgYiKioKCxduhRXr16VFl1MSUnB9OnTsWrVKgwaNAjZ2dkAAC8vL2i1WgDAihUrMGjQIHTp0gU6nQ5vvvkm0tLS8M477zTnUm1LXwSc3Aj4hDU5AAWxBYiIiMhmmtUFlp+fj9jY2FrHY2NjkZ+f3+jznDhxAn369JFmji1atAh9+vTBc889BwDIyspCRkaGVP7dd9+F0WjEvHnzEB4eLm1PPvmkVKagoABz5sxBt27dkJiYiKtXryI1NdU5btIa2gNQKIHibKAou0lvtcwCyy81wGRu0bAtIiKiNq9Zg6Dj4+MRHx9fa1DxggULcOzYMRw9etRmFZRDUwZRNdk78cCNs8BDnwG3Jzb6bUaTGV2e+QKCABxfNgrtfFW2rRcREZGLa8rf72Z1gb3yyisYP3489uzZg4SEBCgUChw+fBiZmZnYtWtXsyrdZoTHiQEo63STApC7mxIBGk/klxiQV6JnACIiImqBZnWB3XXXXfjll19w3333oaCgAPn5+Zg8eTJ++uknbNy40dZ1bF3C48THrLQmv5UzwYiIiGyjWS1AABAREVFrsPPp06fx73//Gxs2bGhxxVotKQCdafJbxXFAxcjlatBEREQt0qwWIGqBsMrp+IUZQGnjB4wDnAlGRERkKwxAjqbWAoGdxOdZp5v0VstaQGwBIiIiahkGIDlI3WBNC0BB3mwBIiIisoUmjQGaPHlyg68XFBS0pC5tR3gc8NO2prcA+XI1aCIiIltoUgCyrLbc0OvTp09vUYXahBa2APGO8ERERC3TpADEKe42ElYZgPJ/Bcp1gLpxiy0G8X5gRERENsExQHLwDgK0keLz7B8a/TbLOkC5RWwBIiIiagkGILk0oxvM0gJUVmFCqcFoj1oRERG1CQxAcmlGAPL2dIPaQ/xHxplgREREzccAJJdmBCCFQiHdFZ5rARERETUfA5BcLAEo9xxgKG3023g/MCIiopZjAJKLbxjgEwoIZuD6T41+WxBXgyYiImoxBiA5NePO8NJq0CVsASIiImouBiA5NWMckGU1aLYAERERNR8DkJyaMxWe9wMjIiJqMQYgOVkCUE46YGxci04wV4MmIiJqMQYgOWkjAa8AwFwhhqBGCOJq0ERERC3GACQnhaLJ3WCWdYDYAkRERNR8DEBya2IACvYVW4DySwwwmQV71YqIiKhVYwCSWxMDUKBGDEBmASgoZTcYERFRczAAyS38DvHx+o+A6dY3OHV3UyJA4wGAawERERE1FwOQ3AJiAE9fwFgO5P7SqLdIq0EXcRwQERFRczAAyU2pBMJ7i88bPRC6ciYYW4CIiIiahQHIGTR1ILRlLSCuBk1ERNQsDEDOoMkBiKtBExERtQQDkDOwBKDsM4DZfMviQVwNmoiIqEUYgJxBUBfA3QswFAP5v926uGU1aLYAERERNQsDkDNwcwfCeorPs9JuWdyyGjTvCE9ERNQ8DEDOognjgDgGiIiIqGVkDUCpqamYOHEiIiIioFAosH379lu+58CBA+jXrx/UajU6deqEtWvX1iqzZcsWdO/eHSqVCt27d8e2bdvsUHsba1IA4iwwIiKilpA1AJWUlCAuLg5vv/12o8pfvHgR48aNw9ChQ3Hq1Ck8/fTT+POf/4wtW7ZIZY4cOYKpU6di2rRpOH36NKZNm4YpU6bg6NGj9roM26gegISG7/FlGQNUYjChzGCyd82IiIhaHYUg3OKvrYMoFAps27YNkyZNqrfM3/72N+zcuRPp6enSsblz5+L06dM4cuQIAGDq1KnQ6XT44osvpDJjxoxBQEAAUlJSGlUXnU4HrVaLwsJC+Pn5Ne+CmspoAF6KAMwVwJNngIDoeosKgoCuz34Jg9GMQ38bgQ4BGsfUkYiIyIk15e+3S40BOnLkCBITE62O3XPPPThx4gQqKioaLHP48GGH1bNZ3D2B0O7i81t0gykUCgR7cyYYERFRc7lUAMrOzkZoaKjVsdDQUBiNRuTm5jZYJjs7u97z6vV66HQ6q00WTRgHFMRxQERERM3mUgEIEFs/qrP04FU/XleZmseqS0pKglarlbbIyEgb1rgJOBOMiIjIIVwqAIWFhdVqycnJyYG7uzuCgoIaLFOzVai6pUuXorCwUNoyMzNtX/nGCL9DfMxKa8RA6Mq1gLgaNBERUZO5VABKSEjA7t27rY59/fXX6N+/Pzw8PBosM3jw4HrPq1Kp4OfnZ7XJIrQHoHADSm4ARfV32QFVM8HYAkRERNR0sgag4uJipKWlIS0tDYA4zT0tLQ0ZGRkAxJaZ6dOnS+Xnzp2Ly5cvY9GiRUhPT8eGDRuwfv16LF68WCrz5JNP4uuvv8bKlStx9uxZrFy5Env27MHChQsdeWnN4+EFtOsqPr9FN1i7yhagX64X2btWRERErY6sAejEiRPo06cP+vTpAwBYtGgR+vTpg+eeew4AkJWVJYUhAIiJicGuXbuwf/9+3HHHHXjxxRfx5ptv4v7775fKDB48GJ988gk2btyI3r17Izk5GZs3b0Z8fLxjL665GjkO6O7YECgUwMHzuUjPkmnQNhERkYtymnWAnIks6wBZfLcG+HIJ0HU88ODHDRad9/H3+PxMFibGReCtB/s4qIJERETOqdWuA9QmNGEm2LzhtwEAPj9zDRdzS+xZKyIiolaFAcjZhPUSH3VXgJLcBot2j/DD3bEhMAvA2v2/OqByRERErQMDkLNR+QJBYstOo1qBRohlt566gmsFZfasGRERUavBAOSMmtAN1i86AIM6BaLCJGBd6m92rhgREVHrwADkjJoQgABg/oguAIBPjmcgl7fGICIiuiUGIGfUxAA05LYgxHXQorzCjA2HLtqxYkRERK0DA5AzCustPt68CJQV3LK4QqGQxgJ9cOQyCssq7Fg5IiIi18cA5Iw0gYB/lPg8+4dGvWVUt1DcHuqDIr0RHxy5ZL+6ERERtQIMQM6qid1gSmVVK9CGby+h1GC0V82IiIhcHgOQs2piAAKA8b3CERWoQX6JASnHZLqjPRERkQtgAHJW4XeIj00IQO5uSvxpeGcAwHupv0FvNNmhYkRERK6PAchZWVqAcn8BDI2/zcXkvu0R6qdCtq4cW7+/aqfKERERuTYGIGflEwL4hgMQgOwfG/02lbsbHh/aCQCw9sCvMJrMdqogERGR62IAcmbNGAcEAA/FRyFA44HLeaX4/IcsO1SMiIjItTEAObNmBiCNpzseGxIDAFi971eYzYKta0ZEROTSGICcWTMDEABMH9wRPip3nLtehG/O5ti4YkRERK6NAciZWQLQjXSgorxJb9V6eWBaQjQA4O19FyAIbAUiIiKyYAByZn7tAU0QYDYCOT83+e2z7oyByl2J05kFOPxrnh0qSERE5JoYgJyZQtGibrBgHxUeHCjeUuPtvRdsWTMiIiKXxgDk7FoQgABgzrBOcFcqcOS3PJy8fNOGFSMiInJdDEDOroUBKMLfC5P7tgcArN7HViAiIiKAAcj5WQLQ9Z8AU0WzTvGn4bdBqQC+OZuDn6/pbFg5IiIi18QA5OwCYgCVFjDpgRvnmnWKmGBvjOsVDgBYvZ+tQERERAxAzk6hAMJ7i8+b2Q0GAPNG3AYA+PyHLPx2o9gWNSMiInJZDECuoIXjgACgW7gfRsaGQBDEe4QRERG1ZQxArsAGAQgA5t0ttgJt/f4qrhaUtbRWRERELosByBVYAlD2D4DZ1OzT9I0KQEKnIBjNAt5L/c1GlSMiInI9DECuIOg2wEMDVJQAeS3rvrKMBfrkeAZyi/W2qB0REZHLYQByBUo3IKyX+LyF3WBDbgtCXKQ/yivM2HDoog0qR0RE5HoYgFyFNA4orUWnUSgUmDe8MwDggyOXUVjWvLWFiIiIXBkDkKuw0UBoABjVLRRdQ31RpDfigyOXWnw+IiIiV8MA5CqkAHQGEIQWnUqpVOCJEWIr0PpDF1FqMLa0dkRERC6FAchVtIsF3DwBfSFw81KLTze+VziigzS4WVqBlGOZLa8fERGRC5E9AK1evRoxMTFQq9Xo168fDh48WG/ZmTNnQqFQ1Np69OghlUlOTq6zTHl5uSMux37cPIDQyuu0QTeYu5sSc+8SW4HWpf4KvbH50+uJiIhcjawBaPPmzVi4cCGWLVuGU6dOYejQoRg7diwyMjLqLL9q1SpkZWVJW2ZmJgIDA/H73//eqpyfn59VuaysLKjVakdckn3ZcBwQAEzu2x5hfmpc1+mx9furNjknERGRK5A1AL3++uuYNWsWZs+ejW7duuGNN95AZGQk1qxZU2d5rVaLsLAwaTtx4gRu3ryJRx991KqcQqGwKhcWFuaIy7E/GwcglbsbHh/WCYB4ewyjyWyT8xIRETk72QKQwWDAyZMnkZiYaHU8MTERhw8fbtQ51q9fj1GjRiE6OtrqeHFxMaKjo9GhQwdMmDABp06davA8er0eOp3OanNK1QNQCwdCWzw4MBKB3p64nFeKl3adhclsm/MSERE5M9kCUG5uLkwmE0JDQ62Oh4aGIjs7+5bvz8rKwhdffIHZs2dbHY+NjUVycjJ27tyJlJQUqNVqDBkyBOfPn6/3XElJSdBqtdIWGRnZvIuyt5AegMINKM0FdNdsckqNpzv+kng7AGDDtxcxc+MxFJQabHJuIiIiZyX7IGiFQmG1LwhCrWN1SU5Ohr+/PyZNmmR1fNCgQXjkkUcQFxeHoUOH4tNPP8Xtt9+Ot956q95zLV26FIWFhdKWmemks6I81EBIN/G5jbrBAODh+Gi89WAfeHm44eD5XEx8+xDSs5y0FYyIiMgGZAtAwcHBcHNzq9Xak5OTU6tVqCZBELBhwwZMmzYNnp6eDZZVKpUYMGBAgy1AKpUKfn5+VpvTsvE4IIuJcRHY+sRgRAZ6ITO/DJNXH8Z/T9umlYmIiMjZyBaAPD090a9fP+zevdvq+O7duzF48OAG33vgwAFcuHABs2bNuuXnCIKAtLQ0hIeHt6i+TsNOAQgAuoX74b/z78TQLsEoqzBhQcopJO1K57ggIiJqdWTtAlu0aBHef/99bNiwAenp6XjqqaeQkZGBuXPnAhC7pqZPn17rfevXr0d8fDx69uxZ67UVK1bgq6++wm+//Ya0tDTMmjULaWlp0jldnh0DEAD4azyR/OhA/PEucXbYu6m/cVwQERG1Ou5yfvjUqVORl5eHF154AVlZWejZsyd27dolzerKysqqtSZQYWEhtmzZglWrVtV5zoKCAsyZMwfZ2dnQarXo06cPUlNTMXDgQLtfj0OE9gSgAIquAcU5gE+IzT/CTanA0rHd0DNCi7/+54w0LujdR/qje4QTdw8SERE1kkIQbDSfuhXR6XTQarUoLCx0zvFAbw8Acn8BHt4CdBll149Kz9Lhjx+cREZ+KdQeSrzyQBx+Fxdh188kIiJqjqb8/ZZ9Fhg1g9QNlmb3j+oW7oed84dgaJdglFeY8efKcUFcNJGIiFwZA5ArsvM4oJos44Is9w4TxwUdx80SjgsiIiLXxADkihwcgABxXNCSsbF4+yFxvaBDF3Lxu3cO4edrXC+IiIhcDwOQKwrrLT4WXAbKbjr0oyf0FtcLigrUiOsFrfkWO7leEBERuRgGIFfk5Q/4V97/LOuMwz/eMi5o2O3tpHFBL3FcEBERuRAGIFclQzdYdf4aT2ycOQB/Gi6OC1rHcUFERORCGIBcVUQf8fHQv4BzX8pSBTelAn8bE4t3HuorjQua+DbHBRERkfNjAHJV/WaKY4HK8oGUqcAXfwOMelmqMr53OLbNE8cFXblZhkmrv0XSF+koLK2QpT5ERES3wgDkqjSBwOw9wKAnxP2ja4H3RwK59d/01Z5iw8RxQSO6toPBaMa7B37DsH/uw7rUX1FeYZKlTkRERPXhStB1cPqVoGv65Stg+5+A0jzAQwOM+ydwx8OAQuHwqgiCgH3ncrDyi3M4d70IABChVWNRYlfc16c93JSOrxMREbUNTfn7zQBUB5cLQACgywK2zQEupor7PR8AJrwOqLWyVMdkFrD1+yt4ffcvyCosBwB0DfXF38Z2xYiuIVDIEM6IiKh1YwBqIZcMQABgNgHfvgHs/QcgmMSp8g9sBDr0k61K5RUmbDpyCe/s+xWFZeKYoPiYQCwZG4s+UQGy1YuIiFofBqAWctkAZJF5DPjPLKAwA1C6A3c/Cwz+M6CUb8hXYWkFVh+4gI3fXoLBKK4XNLZnGBbf0xWd2/nIVi8iImo9GIBayOUDEACUFQD/Wwj8tE3c7zQCuO9dwDdUzlrhWkEZ/rX7F2z5/grMgjiV/g8DIvHkyC4I8VPLWjciInJtDEAt1CoCEAAIAvD9psop8mWAdztg0lqgyyi5a4Zz2UX451dnsSc9BwDg5eGGx4fG4PFhneCr9pC5dkRE5IoYgFqo1QQgixvngP88Blz/UdxPmA+MfB5w95S3XgCO/paHl788i1MZBQCAQG9PLLj7NjwcHw1Pd67SQEREjccA1EKtLgABQEU5sPtZ4Ng6cT/8DuCBDUBQZ1mrBYhT57/6KRuvfHkOv+WWAAAiA72wOLErJvaOgJJT54mIqBEYgFqoVQYgi7OfAzvmiXeR9/QBxr8OxE2Vu1YAAKPJjE9PXMG/9vyCG0Xiqtbdwv0wb0RnjO0ZzjWEiIioQQxALdSqAxAAFF4Ftj4OXP5W3I97UFw8UeUrb70qlRqM2HDoItYe+A3FeiMAICbYG3+6qzMm9WnPrjEiIqoTA1ALtfoABIhrBqW+Chx4GRDMQEAM0OdhIOYu8UarbvIPRC4oNSD58CVs/PaStIZQuFaNOcM64Q8DouDl6SZzDYmIyJkwALVQmwhAFpePAFtmA7orVcc8fYCoBCBmmLiF9QKU8oWNYr0RKUczsO7gb1LXWJC3Jx67MwaPDIqG1kv+sEZERPJjAGqhNhWAAHHNoB8+E2+jcemgOD6oOrUW6DhUDEMdhwIh3WS5z1h5hQlbvr+CtQd+RWZ+GQDAV+WOaQnReOzOGAT7qBxeJyIich4MQC3U5gJQdWYzkPOTGIYupgKXvgUMRdZlvNtVBaKYYUBgJ4cGIqPJjP+euYbV+37F+ZxiAIDaQ4k/DIjCnGGdEOHv5bC6EBGR82AAaqE2HYBqMhmBrNPAxQNiIMr4TlxUsTq/9lVhqONQwD/SIVUzmwXsTr+O1fsu4PSVQgCAu1KByX3bY+5dndGJt9ggImpTGIBaiAGoAUY9cPVkVQvRleOAyWBdJqS7OKus450OqZIgCPj2Qh5W77+Aw7/mARAbpMb1CscTwzujR4TWIfUgIiJ5MQC1EANQExhKgcyjVeOHrn4v3okeCiB+LjDyOcBT47DqfJ9xE6v3/Yo96delYyO6tsMTI27DgI6BDqsHERE5HgNQCzEAtUDZTWD3c+I9yAAgsDMwaQ0QFe/QapzN1mHN/l/x39PXYK78hXcL98O4nmEY2ysct4Wwe4yIqLVhAGohBiAbOL8H2LkAKLoGKJTi/cdGLAM8HHvH90u5JXg39TdsOXkFBpNZOt4lxAdje4VjXK8wdA31hUKGWW1ERGRbDEAtxABkI2UFwJdLgdMfi/vBXYH71gDt+zm8KjdLDNj983Xs+jEL317IRYWp6mcfE+yNsT3DMLZnOHq292MYIiJyUQxALcQAZGNndwH/fRIoyQEUbsCdC4G7/ga4y7NuT2FZBb5Jv44vfszGgV9uwGCsahnqEOAlhqFe4bijgz9vxEpE5EIYgFqIAcgOSvOBXf8H/PgfcT+kh9gaFB4na7WK9UbsO5uDL37Mwr6zN1BWYZJeC/NTY0zPMIztGYb+HQN5M1YiIifHANRCDEB29PMO4H9PAaV5gNIdGPZ/wNC/OMW9x8oMJhz4JQdf/JiNb9JzpBuxAkCwjwr39AjFuF7hiI8JhLsbb8hKRORsmvL3W/b/iq9evRoxMTFQq9Xo168fDh48WG/Z/fv3Q6FQ1NrOnj1rVW7Lli3o3r07VCoVunfvjm3bttn7Mqixut8LPHEU6PY7wGwE9icB790NXP9J7prBy9MNY3qGY9Uf+uDEM6OwfkZ/3N+3A/zU7sgt1uOjoxl4+P2jGPCPPZj30ffYdOQSzmUXwWzm/0MQEbkaWVuANm/ejGnTpmH16tUYMmQI3n33Xbz//vv4+eefERUVVav8/v37MWLECJw7d84q2bVr1w5ubuLNOo8cOYKhQ4fixRdfxH333Ydt27bhueeew6FDhxAf37ip2GwBcgBBAH7cAuxaLE6dV3oAI5YCg58E3Nzlrp0Vg9GMI7/l4csfs/DVT9eRX2K98GOAxgMDYwIRHxOE+E6B6Bbmx7FDREQycJkusPj4ePTt2xdr1qyRjnXr1g2TJk1CUlJSrfKWAHTz5k34+/vXec6pU6dCp9Phiy++kI6NGTMGAQEBSElJaVS9GIAcqOg68L+FwLld4n77fuK6Qe26ylqt+hhNZpzKLMB3v+bh6MV8nLx802rcEAD4qd2tAlH3cD92mREROUBT/n7L9r/aBoMBJ0+exJIlS6yOJyYm4vDhww2+t0+fPigvL0f37t3xzDPPYMSIEdJrR44cwVNPPWVV/p577sEbb7xR7/n0ej30er20r9PpmnAl1CK+ocAfPgbObAZ2/VW8zcbaocDdzwAJ8wClm9w1tOLupsSAjoEY0DEQCyC2Dv1wtRBHL+bh6G/5OHEpH7pyI/ak52BPeg4AwEfljv4dA6RA1Ku9Fh4MREREspItAOXm5sJkMiE0NNTqeGhoKLKzs+t8T3h4ONatW4d+/fpBr9fjgw8+wMiRI7F//34MGzYMAJCdnd2kcwJAUlISVqxY0cIromZTKIC4P4g3U925ALiwB9j9LHD2f8D414DQng6923xTeLor0S86AP2iA/DEcLGF6KdrOikQHbuUj6JyI/afu4H9524AADSebugXHYD4mEDEdwpCr/ZaqD2cK+gREbV2sg+2qLnonCAI9S5E17VrV3TtWtU1kpCQgMzMTLz66qtSAGrqOQFg6dKlWLRokbSv0+kQGemYO5pTNX4RwMP/AU59AHz5tHiPsbV3At7tgKhBQNRgIDoBCO3ldOOELNzdlIiL9EdcpD/mDOsMk1lAepYORy/m4+hveTh2KR8FpRU4eD4XB8/nAgA83BToHqFFn0h/9I0OQN8of7T39+KCjEREdiTbX5Hg4GC4ubnVapnJycmp1YLTkEGDBuHDDz+U9sPCwpp8TpVKBZVKnkX5qAaFAug7Heg0HPhiidgaVHIDSP+vuAGApw/QYQAQPRiIShDHDTnwhqtN4aZUoGd7LXq212LWnTEwmwX8klOEo7/l4+jFPBy7eBO5xXqczizA6cwCJB++BAAI8VWhT5Q/+kYFoG90AFuJiIhsTPZB0P369cPq1aulY927d8e9995b5yDoujzwwAPIz8/H3r17AYiDoIuKirBr1y6pzNixY+Hv789B0K7IqAeunQIuHwYyjgAZRwF9oXUZpQcQcYcYhqISxNYijWvc+V0QBFy5WYbvM27iVEYBvs+4iZ+v6WCsMbXeXalA9wg/9I0KkIJRhwC2EhERVecys8As0+DXrl2LhIQErFu3Du+99x5++uknREdHY+nSpbh69So2bRLvLP7GG2+gY8eO6NGjBwwGAz788EO8/PLL2LJlCyZPngwAOHz4MIYNG4Z//OMfuPfee7Fjxw4888wznAbfWphNQE66GIYsoagoq3a5dt3E7rKowWIg8nedLs0ygwk/XivE95dv4vuMm/g+owA3ivS1yrXzVVXrNhNbibw82UpERG2XS8wCA8TWmry8PLzwwgvIyspCz549sWvXLkRHRwMAsrKykJGRIZU3GAxYvHgxrl69Ci8vL/To0QOff/45xo0bJ5UZPHgwPvnkEzzzzDN49tln0blzZ2zevLnR4YecnNINCOspbgMfF9cTKrgMXD4CZBwWH/POAzfSxe3EBvF92kggMh6IHCh2n4X1corVp+vi5ekmzTQDqlqJTmUW4PvLN3Eq4yZ+uqbDjSI9vv75Or7++ToAQKkAOgZ7o1uYH2LDfNEt3A+x4b4cT0REVAfeCqMObAFyccU3gMzvqkJR1hlAsF6rB+5eQEQfIHIA0GGgGIx8QuSpbzOUV5jww9VCnMq4ie8vi11nOXW0EgGAr8odseG+iA3zkx67hvnCR+WcA8mJiJrLZbrAnBUDUCujLwauHBe3zKPiY3lh7XIBHavCUIcB4vR7J51tVpMgCLhRrMfZrCKczdYhPasI6Vk6/HqjGBWmuv8VjwrUIDbMF7HhfuheGYyiAjVcxZqIXBYDUAsxALVyZrPYTZZ5DLhyDMg8Dtw4C6DGvwoeGiCir3UrkXewLFVuLoPRjN9yi3E2qwjp2TopIF3X1d1a5OXhhq5hvrgtxAcxwd7oGOSNjsEadAzyhjdbjIjIyTEAtRADUBtUXghcOVHZSnRMfF5zthkABMSIQSg8TmwhCu0JeAc5vr4tlFesx7nsIqRnF+Fslg5ns4tw7noRDEZzve9p56tCTJA3ooM06BjsjZjgyucMR0TkJBiAWogBiGA2A7nnrFuJcs/VXdY3XAxCYT2rQlHQbS7TfWZhNJlxKa8U6Vk6XMwtwaXcElzMK8HlvNJaN4CtKcRXJbUWRQd5S61H0UEahiMichgGoBZiAKI6ld0ErpwErp4Asn8Arv8I3LxUd1l3NdAutkYw6uEy6xPVVFhagUt5JeKWW1rteQlullY0+N52vip0DNIgKlAMROLmjehADfw1HpyhRkQ2wwDUQgxA1GjlOnFdous/ANk/Atd/EreKkrrL+7W3DkUh3QDvEMDL3+lu/NpYLQlHvmp3dAzyRlSQBtGBmqrnQRqE+qo5IJuImoQBqIUYgKhFzGbg5kWxhej6T5XB6AegIKP+9yiUgFcAoAmqf/MOFluQLPuePk57k1iLwtIKXM4Xu9EuV3anXc4Xn9c3ENtC5a5EVKAYhiytR5ag1D7ACyp31wyMRGQ/DEAtxABEdlFeCFz/WQxG2T+I4SjvfN1T8hvDTVUtIAWKq11HDwE6DnWJla/LDCZk3izFpdwSZOSX4nKe2HqUkV+KKzfLYDLX/58mhQII91MjUgpImsrn3ogK1CCAXWtEbRIDUAsxAJFDmSrE8UWleUBJrvhYmgeU5gOl1fZLcquOGcsbPqd/tBiEOt4pbi4QiKqrMJlxraCsqsUotwSX80uRmV+KjPxSlBpMDb7fR+WOqEAxGEUFVYajyv0Ify94uisddCVE5EgMQC3EAEROz1BaIxzlATk/A5cOiTePrbnydUDHyjBUGYq0HWSpti0IgoDcYgMyqgWiy3lVz7N1DYdDpQII13qhfYAXOviLj+2rPUb4e0Htwe41IlfEANRCDEDk0vRFQMZR4NJBcbuW1kAgGlYZiNrLUVO7KK8w4cpNMQxlVLYgWcJRRn4pyivqX+vIIthHJQakekKSr9o57yNH1NYxALUQAxC1KuU68RYglw5WayGqEQICYmq0ELWeQFSdIAi4UaRH5k1xnNHVgjJcrfF4q+41APBTu6N9gKayxUiNEF8V2lk2HzVC/FQI8vaEuxu72ogciQGohRiAqFUr1wEZ31UFoqy02oFIrQXU/uLMNK/KR2m/rmOVjx4ap5+Z1hBBEFBQWoGrBWW4UldIKihDwS2m9lsoFECgxrMqGPmqEOKrrva86rivyp2DtolsgAGohRiAqE0pL6wRiE7XDkSNpfSwDkRWW6B4XBNYtW957gJT+i1K9EYpFF0pKMP1wnLcKNIjp6gcN4r1uFGkR26xocFZbDWpPZRo56tCoMYTfl4e4qb2gNbLA35e7uJj5b628nWtlwd81e7wYCsTkYQBqIUYgKhNK9cBRdnizLTyAvGxrODW++bGtYzUyRKcqocjrwBAUy08aYLEhST9IwHvdk4dmExmATdLDZXBSAxFUkiqtn+jSI8ivbFFn+Xt6SYFIj91VTgK9hFbn0L81Gjno0KIn9jq5MPWJmrFGIBaiAGIqIkEATCU1A5IVlu+OI2/rEB8XnZT3Dc1vCBindzV4kw2baQYiPyjAG2U+FwbKd6fzUXuxVZmMEnhqKC0ArryChSWiZuuzCg+llv2K7dyI4qbGZzUHkqpKy6kWldciK8a7fxUUlgK8lbBjStxk4thAGohBiAiBxEEoKLMOhBZPa8WoEpygcIrQFEWgFv8Z0vhVtVaZAlJ0mOUGJ481A65RHsxmswoKjdWhaXyqtBUUGZAbpGhVotTU1qblAogyEcMREE+ngj0rtw0ngj08USQtycCNJ6Vr6mg9fJgYCLZNeXvt2v8LxIRtU4KBeCpEbfGrk1kNAC6K0BBJlCYWe0xQ3wsvCp2xxVmiFt9fMLElqOAaPHRP7pq368D4O5pm2u0E3c3JQK8PRHg3fh6lhqM1brjqrrlcnR63CjWS495xXqYBUhlG0OpAPw11UKStxiULM+DfMTAFKDxRIC3BwI0ntB4urE7jmTDFqA6sAWIyIWZzUBxdu1gVD0w1XezWokC8IuwDkXVQ5Jfe5fpYmsOo8mM/BKDGJKK9cgvNuBmqQF5JQbkFxuQX2pAfknVVljWvPFfnm5KKQz5azwQ6O0Jf40nAjQetcKSZfNVu/MmuVQvdoG1EAMQUSsmCGL3WsFlMRxZHm9a9jMAY1nD51C4iWsl+UeLA7LdPMSB3G4e1Z67A26eVc9rvV7He1Q+1jPm3FUO+UpaqsJkxs1SA26WVCCvRI/8EgNuloiByfJoCUsFpRXILzXAYGzeTEM3pQL+1WbB+ajd4aNyh49K3Pe17Fc++qk9qpUR971VblyjqZViAGohBiCiNkwQgJIblaHoUu2QVJgJmAyOqYuHd+21l6xmy9W1BTr9+CZBEFBWYcLN0grcLBFbl6yel1TulxqkYFVQakBJIxapbCwvDzf4qN3hq3KHr5cH/NTu0vIDfl7u0ow6y3Ftjdd4uxTnxADUQgxARFQvqYutMhCV3RTHHJkMgMlY+bxyM9d4lJ4b63iPAdAXVy0v0Ny1mABxlpzaH1D7iYtaqiofa+3XU8ZJ12XSG01iC1KJAbqyChTrxdlwReWWxwoUlxtRpDeiuLzma0YU6ysadSuUxvB0V9YZliwtTd7VHr1VbvBVu8Pb0/q4r9odKnclx0HZEANQCzEAEZGszGZAX1hjGYGC2ksL1JwpV3az9n3fmkPhBqh8qwKRhwbw8Kr2WO25u1f9r9VV1l0lBjSZxlAZjGaUVAtOReUVKCo3QldetcSArtrMOl1Z5WvVntvyr6abUgFvT7eq0FQtRFnGQ1mNjaqcfRfI8VB14iwwIiJXplRWdWk1hSCIN8MtyxcXtCwvBPSVj5b98kIxXNU6VvncbBRDVHmBuNmLwk0MQpZAVOejqo7jlc89fQDvYEATLC6S6R0sdg2q/RtsvfJ0V8LTvWmz56ozmwWUGIzSkgPVg1NhWQVK9EYpYFkexecmlOjF1qkSvVG655zJLIihq7zp6zpZZt5ZBo2Ls/AqB417i8f9NZ7QenlA4+kGjacbvDzd4eUhPm/rrU8MQERErYVCUdml1cyWa8u6TDWDU0UpYCwXHyvKqj1W3+p4zVjjtepjpwSTOBvvljPymkjpLgYiy+ZdGZA0wVUhySo0BQFQ1HNtta9bWVEG34pS+Fpdd2UZk0EcFO8fDXSoNnPQJ1QMtdWYzAJKDWIwKtZXoLgyIFmCU1G5ETdLDVKXX13jocwCpMHlQNO/R6VCHAvl5ekOL08lNB7u8KoMSlVhSQmNZ+VxDzd4ebpB7eFW+T7xUV3tubivhLpy35lv1cIAREREourrMiHc9uc3mwCjXgwVDT2a9NX26yhTUS62dJXmigtkluaJm6FYbMEqvi5uzsJNVbVieeVSCm7+UfAN6Ahf/yigXdNv7WIZD3WztNrsuhIDCmoOKK9cXbzMYEKpwYQygwkGkzgOyiwAJQaTTQeX1+SuVIihqHpA8nSD2l2JbuF+WP67Hnb77FvWTbZPJiKitkXpVi1g2UFFeWUYqgxFJXlV+9WDkuV5WX6NweYKwNO7auxSg+Obao5z8hKXMijKtp41qLsiBrq8C+JWF3ev2utN+UWI53TzrFwqQVXtuSdU7p4IdfNEqI8noPUA3DSVyy643TJMGU1mlFWYpFBUajBV2zeirKIqLInPjdJ+eUVl2Qozyi3vq6j5mkkaJ2U0Cyiq7PqrqSk3DLYHBiAiImodPNTi+kza9o0rbzaJ3XwKhRho3DxtP/vNVAHortWx3lTlo+6a2FWYe07cWkwhXoe7SgpLNdeccld6wNfNA75unmKXoaVcQ881HoCvpxgQrW5Y3K5q3So3DwDiMgcGkxnlBrNVQCqrEEOSJSj5qj1scL3NxwBERERtk9JNHBNkT24eYstOQHTdrxv14j3uaq43VZQtthyZDJVLKBjE28CYDNWO6etYk0qoPN6Mmwy3lMoP8PKHwisQKq8AqDSB0EpBKbAqOPlajnk5vo7VMAARERHJxV0FBHUWt+YQBHHck8lQOX6qoo6QVH0NKkP961DV+7yi6jP0xbVvWGyZLajXiVtBA/fgqy6sFzD3UPOu2wYYgIiIiFyVQlF1OxVPb3nqYDZVW6eqejiqEZSk/crnXnZufbsFBiAiIiJqPqUb4B0kbk1htt/ss8aQfYL+6tWrERMTA7VajX79+uHgwYP1lt26dStGjx6Ndu3awc/PDwkJCfjqq6+syiQnJ0OhUNTaysvL7X0pRERE1FhKee+nJmsA2rx5MxYuXIhly5bh1KlTGDp0KMaOHYuMjLr7D1NTUzF69Gjs2rULJ0+exIgRIzBx4kScOnXKqpyfnx+ysrKsNrXauW8OSERERI4j673A4uPj0bdvX6xZs0Y61q1bN0yaNAlJSUmNOkePHj0wdepUPPfccwDEFqCFCxeioKCg2fXivcCIiIhcT1P+fsvWAmQwGHDy5EkkJiZaHU9MTMThw4cbdQ6z2YyioiIEBloPpCouLkZ0dDQ6dOiACRMm1GohIiIiorZNtgCUm5sLk8mE0NBQq+OhoaHIzs5u1Dlee+01lJSUYMqUKdKx2NhYJCcnY+fOnUhJSYFarcaQIUNw/vz5es+j1+uh0+msNiIiImq9ZJ8FVvNOtIIgNOrutCkpKVi+fDl27NiBkJAQ6figQYMwaNAgaX/IkCHo27cv3nrrLbz55pt1nispKQkrVqxo5hUQERGRq5GtBSg4OBhubm61WntycnJqtQrVtHnzZsyaNQuffvopRo0a1WBZpVKJAQMGNNgCtHTpUhQWFkpbZmZm4y+EiIiIXI5sAcjT0xP9+vXD7t27rY7v3r0bgwcPrvd9KSkpmDlzJj7++GOMHz/+lp8jCALS0tIQHl7/nY1VKhX8/PysNiIiImq9ZO0CW7RoEaZNm4b+/fsjISEB69atQ0ZGBubOnQtAbJm5evUqNm3aBEAMP9OnT8eqVaswaNAgqfXIy8sLWq0WALBixQoMGjQIXbp0gU6nw5tvvom0tDS888478lwkEREROR1ZA9DUqVORl5eHF154AVlZWejZsyd27dqF6GjxpnFZWVlWawK9++67MBqNmDdvHubNmycdnzFjBpKTkwEABQUFmDNnDrKzs6HVatGnTx+kpqZi4MCBDr02IiIicl6yrgPkrLgOEBERketxiXWAiIiIiOTCAERERERtDgMQERERtTmyL4TojCzDorgiNBERkeuw/N1uzPBmBqA6FBUVAQAiIyNlrgkRERE1VVFRkbQ8Tn04C6wOZrMZ165dg6+vLxQKBXQ6HSIjI5GZmclZYQ7E710e/N7lwe9dHvze5WGv710QBBQVFSEiIgJKZcOjfNgCVAelUokOHTrUOs5VouXB710e/N7lwe9dHvze5WGP7/1WLT8WHARNREREbQ4DEBEREbU5DECNoFKp8Pzzz0OlUsldlTaF37s8+L3Lg9+7PPi9y8MZvncOgiYiIqI2hy1ARERE1OYwABEREVGbwwBEREREbQ4DEBEREbU5DEC3sHr1asTExECtVqNfv344ePCg3FVq9ZYvXw6FQmG1hYWFyV2tVic1NRUTJ05EREQEFAoFtm/fbvW6IAhYvnw5IiIi4OXlheHDh+Onn36Sp7KtyK2+95kzZ9b6/Q8aNEieyrYSSUlJGDBgAHx9fRESEoJJkybh3LlzVmX4e7e9xnzvcv7eGYAasHnzZixcuBDLli3DqVOnMHToUIwdOxYZGRlyV63V69GjB7KysqTthx9+kLtKrU5JSQni4uLw9ttv1/n6K6+8gtdffx1vv/02jh8/jrCwMIwePVq6Vx41z62+dwAYM2aM1e9/165dDqxh63PgwAHMmzcP3333HXbv3g2j0YjExESUlJRIZfh7t73GfO+AjL93geo1cOBAYe7cuVbHYmNjhSVLlshUo7bh+eefF+Li4uSuRpsCQNi2bZu0bzabhbCwMOHll1+WjpWXlwtarVZYu3atDDVsnWp+74IgCDNmzBDuvfdeWerTVuTk5AgAhAMHDgiCwN+7o9T83gVB3t87W4DqYTAYcPLkSSQmJlodT0xMxOHDh2WqVdtx/vx5REREICYmBn/4wx/w22+/yV2lNuXixYvIzs62+v2rVCrcdddd/P07wP79+xESEoLbb78djz/+OHJycuSuUqtSWFgIAAgMDATA37uj1PzeLeT6vTMA1SM3NxcmkwmhoaFWx0NDQ5GdnS1TrdqG+Ph4bNq0CV999RXee+89ZGdnY/DgwcjLy5O7am2G5TfO37/jjR07Fh999BH27t2L1157DcePH8fdd98NvV4vd9VaBUEQsGjRItx5553o2bMnAP7eHaGu7x2Q9/fOu8HfgkKhsNoXBKHWMbKtsWPHSs979eqFhIQEdO7cGf/+97+xaNEiGWvW9vD373hTp06Vnvfs2RP9+/dHdHQ0Pv/8c0yePFnGmrUO8+fPx5kzZ3Do0KFar/H3bj/1fe9y/t7ZAlSP4OBguLm51Ur/OTk5tf4vgezL29sbvXr1wvnz5+WuSpthmXXH37/8wsPDER0dzd+/DSxYsAA7d+7Evn370KFDB+k4f+/2Vd/3XhdH/t4ZgOrh6emJfv36Yffu3VbHd+/ejcGDB8tUq7ZJr9cjPT0d4eHhclelzYiJiUFYWJjV799gMODAgQP8/TtYXl4eMjMz+ftvAUEQMH/+fGzduhV79+5FTEyM1ev8vdvHrb73ujjy984usAYsWrQI06ZNQ//+/ZGQkIB169YhIyMDc+fOlbtqrdrixYsxceJEREVFIScnB3//+9+h0+kwY8YMuavWqhQXF+PChQvS/sWLF5GWlobAwEBERUVh4cKFeOmll9ClSxd06dIFL730EjQaDR566CEZa+36GvreAwMDsXz5ctx///0IDw/HpUuX8PTTTyM4OBj33XefjLV2bfPmzcPHH3+MHTt2wNfXV2rp0Wq18PLygkKh4O/dDm71vRcXF8v7e5dl7pkLeeedd4To6GjB09NT6Nu3r9X0PbKPqVOnCuHh4YKHh4cQEREhTJ48Wfjpp5/krlars2/fPgFArW3GjBmCIIhTg59//nkhLCxMUKlUwrBhw4QffvhB3kq3Ag1976WlpUJiYqLQrl07wcPDQ4iKihJmzJghZGRkyF1tl1bX9w1A2Lhxo1SGv3fbu9X3LvfvXVFZSSIiIqI2g2OAiIiIqM1hACIiIqI2hwGIiIiI2hwGICIiImpzGICIiIiozWEAIiIiojaHAYiIiIjaHAYgIqJGUCgU2L59u9zVICIbYQAiIqc3c+ZMKBSKWtuYMWPkrhoRuSjeC4yIXMKYMWOwceNGq2MqlUqm2hCRq2MLEBG5BJVKhbCwMKstICAAgNg9tWbNGowdOxZeXl6IiYnBZ599ZvX+H374AXfffTe8vLwQFBSEOXPmoLi42KrMhg0b0KNHD6hUKoSHh2P+/PlWr+fm5uK+++6DRqNBly5dsHPnTvteNBHZDQMQEbUKzz77LO6//36cPn0ajzzyCB588EGkp6cDAEpLSzFmzBgEBATg+PHj+Oyzz7Bnzx6rgLNmzRrMmzcPc+bMwQ8//ICdO3fitttus/qMFStWYMqUKThz5gzGjRuHhx9+GPn5+Q69TiKyEYfccpWIqAVmzJghuLm5Cd7e3lbbCy+8IAiCeNfpuXPnWr0nPj5e+NOf/iQIgiCsW7dOCAgIEIqLi6XXP//8c0GpVArZ2dmCIAhCRESEsGzZsnrrAEB45plnpP3i4mJBoVAIX3zxhc2uk4gch2OAiMgljBgxAmvWrLE6FhgYKD1PSEiwei0hIQFpaWkAgPT0dMTFxcHb21t6fciQITCbzTh37hwUCgWuXbuGkSNHNliH3r17S8+9vb3h6+uLnJyc5l4SEcmIAYiIXIK3t3etLqlbUSgUAABBEKTndZXx8vJq1Pk8PDxqvddsNjepTkTkHDgGiIhahe+++67WfmxsLACge/fuSEtLQ0lJifT6t99+C6VSidtvvx2+vr7o2LEjvvnmG4fWmYjkwxYgInIJer0e2dnZVsfc3d0RHBwMAPjss8/Qv39/3Hnnnfjoo49w7NgxrF+/HgDw8MMP4/nnn8eMGTOwfPly3LhxAwsWLMC0adMQGhoKAFi+fDnmzp2LkJAQjB07FkVFRfj222+xYMECx14oETkEAxARuYQvv/wS4eHhVse6du2Ks2fPAhBnaH3yySd44oknEBYWho8++gjdu3cHAGg0Gnz11Vd48sknMWDAAGg0Gtx///14/fXXpXPNmDED5eXl+Ne//oXFixcjODgYDzzwgOMukIgcSiEIgiB3JYiIWkKhUGDbtm2YNGmS3FUhIhfBMUBERETU5jAAERERUZvDMUBE5PLYk09ETcUWICIiImpzGICIiIiozWEAIiIiojaHAYiIiIjaHAYgIiIianMYgIiIiKjNYQAiIiKiNocBiIiIiNocBiAiIiJqc/4fkfDceVaFMIcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = list(range(1, len(eval_losses)+1))\n",
    "\n",
    "plt.plot(X,train_losses)\n",
    "plt.plot(X,eval_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Evaluation loss during epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859614c-2684-49a2-9c92-7685f033364e",
   "metadata": {},
   "source": [
    "### Overfitting fixed when adding dropouts, lower evalutation losses. Probably same accuracy at 88% even adding two conv layers and one more 1d layer. Keep in mind I also added another max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "696c67e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 13242,
     "status": "aborted",
     "timestamp": 1740079523285,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "696c67e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=12544, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ae609a8",
   "metadata": {
    "executionInfo": {
     "elapsed": 13242,
     "status": "aborted",
     "timestamp": 1740079523286,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "9ae609a8"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    prediction_list = []\n",
    "    test_losses = []\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    fmodel.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = fmodel(data)\n",
    "#             prediction = output.max(1 , keepdim = True)[1]\n",
    "            prediction = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "            prediction_list.append(prediction)\n",
    "            correct +=(prediction == target).sum().item()\n",
    "\n",
    "            #loss summatory for Average loss\n",
    "            test_loss += criterion(output, target)\n",
    "\n",
    "        accuracy = correct/10000\n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"/nEval: Average loss: {:.4f},\\tAccuracy: {}/{} ({:.0f}%)\".format(sum(test_losses), correct, 10000, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10175c08",
   "metadata": {
    "executionInfo": {
     "elapsed": 13240,
     "status": "aborted",
     "timestamp": 1740079523286,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "10175c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nEval: Average loss: 0.3301,\tAccuracy: 8781/10000 (88%)\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af9f42-c068-4016-85df-c9849a28af6d",
   "metadata": {
    "executionInfo": {
     "elapsed": 13241,
     "status": "aborted",
     "timestamp": 1740079523287,
     "user": {
      "displayName": "Daniel Yelamos",
      "userId": "15488581616951920799"
     },
     "user_tz": -60
    },
    "id": "abed86e6"
   },
   "source": [
    "# **Things to include**\n",
    "## - Batch normalization\n",
    "## - Image transformation\n",
    "## - Leaky Relu or Elu activation functions\n",
    "## - early stoppage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346c82b-4683-4f90-a986-095d6ba4a851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8726c5-1978-421c-ad13-7ab66b30d900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa6c9f-2f35-4974-894a-6b7abe2c8dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
